{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch import nn,optim\n",
    "from torch.nn import functional as F \n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv(\"newresult.csv\")\n",
    "trainlabel = pd.read_csv(\"newlabel.csv\")\n",
    "traindata = traindata.drop(['Unnamed: 0'],axis = 1).values\n",
    "trainlabel = trainlabel['func'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(traindata, trainlabel, test_size = 0.2, random_state = 1)\n",
    "X_train, Y_train = torch.FloatTensor(X_train), torch.FloatTensor(Y_train)\n",
    "X_test, Y_test = torch.FloatTensor(X_test), torch.FloatTensor(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([340.6172,   3.7677,  -0.8499,  ...,   0.9953,   2.8557,   6.4792])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "\n",
    "#dataloader \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batchsize, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batchsize, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_16d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_16d,self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 2, 2, padding=0, stride=2)\n",
    "        #self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv1d(2, 4, 2, stride=2)\n",
    "        self.conv3 = nn.Conv1d(4, 6, 2, stride=2)\n",
    "        self.conv4 = nn.Conv1d(6, 3, 2, stride=2)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(3, 60)\n",
    "        self.fc2 = nn.Linear(60, 20)\n",
    "        self.fc3 = nn.Linear(20, 1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)  \n",
    "        x = self.flat(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_available\n"
     ]
    }
   ],
   "source": [
    "model = CNN_16d()\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu_available\")\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_16d(\n",
      "  (conv1): Conv1d(1, 2, kernel_size=(2,), stride=(2,))\n",
      "  (conv2): Conv1d(2, 4, kernel_size=(2,), stride=(2,))\n",
      "  (conv3): Conv1d(4, 6, kernel_size=(2,), stride=(2,))\n",
      "  (conv4): Conv1d(6, 3, kernel_size=(2,), stride=(2,))\n",
      "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=3, out_features=60, bias=True)\n",
      "  (fc2): Linear(in_features=60, out_features=20, bias=True)\n",
      "  (fc3): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0301]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(1,1,16)\n",
    "xiao = CNN_16d()\n",
    "xiao(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 0 loss : 449.202\n",
      "train epoch 0 loss : 4030.904\n",
      "train epoch 0 loss : 4740.638\n",
      "train epoch 0 loss : 5511.438\n",
      "train epoch 0 loss : 4265.532\n",
      "train epoch 0 loss : 3860.532\n",
      "train epoch 0 loss : 4744.626\n",
      "train epoch 0 loss : 4006.230\n",
      "train epoch 0 loss : 3738.941\n",
      "train epoch 0 loss : 3169.693\n",
      "train epoch 0 loss : 3638.912\n",
      "train epoch 0 loss : 3137.901\n",
      "train epoch 0 loss : 2858.113\n",
      "train epoch 0 loss : 2454.673\n",
      "train epoch 0 loss : 2932.392\n",
      "train epoch 0 loss : 2597.797\n",
      "train epoch 0 loss : 2447.735\n",
      "train epoch 0 loss : 2161.510\n",
      "train epoch 0 loss : 1815.357\n",
      "train epoch 0 loss : 1893.842\n",
      "train epoch 0 loss : 2107.532\n",
      "train epoch 0 loss : 1611.028\n",
      "train epoch 0 loss : 1589.533\n",
      "train epoch 0 loss : 1881.998\n",
      "train epoch 0 loss : 1554.393\n",
      "train epoch 1 loss : 134.484\n",
      "train epoch 1 loss : 1641.655\n",
      "train epoch 1 loss : 1664.962\n",
      "train epoch 1 loss : 1458.882\n",
      "train epoch 1 loss : 1604.735\n",
      "train epoch 1 loss : 1642.154\n",
      "train epoch 1 loss : 1312.273\n",
      "train epoch 1 loss : 1235.233\n",
      "train epoch 1 loss : 1744.248\n",
      "train epoch 1 loss : 1476.151\n",
      "train epoch 1 loss : 1533.986\n",
      "train epoch 1 loss : 1428.622\n",
      "train epoch 1 loss : 1142.688\n",
      "train epoch 1 loss : 1590.882\n",
      "train epoch 1 loss : 1409.525\n",
      "train epoch 1 loss : 1387.413\n",
      "train epoch 1 loss : 1459.933\n",
      "train epoch 1 loss : 1384.825\n",
      "train epoch 1 loss : 1053.118\n",
      "train epoch 1 loss : 1498.427\n",
      "train epoch 1 loss : 1139.112\n",
      "train epoch 1 loss : 1297.699\n",
      "train epoch 1 loss : 1277.356\n",
      "train epoch 1 loss : 1332.995\n",
      "train epoch 1 loss : 1258.747\n",
      "train epoch 2 loss : 93.012\n",
      "train epoch 2 loss : 1112.656\n",
      "train epoch 2 loss : 989.299\n",
      "train epoch 2 loss : 1229.167\n",
      "train epoch 2 loss : 1290.715\n",
      "train epoch 2 loss : 1058.314\n",
      "train epoch 2 loss : 1139.528\n",
      "train epoch 2 loss : 1221.532\n",
      "train epoch 2 loss : 1095.097\n",
      "train epoch 2 loss : 1098.723\n",
      "train epoch 2 loss : 1017.920\n",
      "train epoch 2 loss : 980.445\n",
      "train epoch 2 loss : 893.714\n",
      "train epoch 2 loss : 964.706\n",
      "train epoch 2 loss : 910.729\n",
      "train epoch 2 loss : 934.881\n",
      "train epoch 2 loss : 1042.113\n",
      "train epoch 2 loss : 978.721\n",
      "train epoch 2 loss : 891.144\n",
      "train epoch 2 loss : 846.970\n",
      "train epoch 2 loss : 916.251\n",
      "train epoch 2 loss : 763.685\n",
      "train epoch 2 loss : 808.208\n",
      "train epoch 2 loss : 734.217\n",
      "train epoch 2 loss : 828.145\n",
      "train epoch 3 loss : 139.300\n",
      "train epoch 3 loss : 813.478\n",
      "train epoch 3 loss : 669.228\n",
      "train epoch 3 loss : 669.297\n",
      "train epoch 3 loss : 599.056\n",
      "train epoch 3 loss : 593.577\n",
      "train epoch 3 loss : 668.992\n",
      "train epoch 3 loss : 622.168\n",
      "train epoch 3 loss : 523.551\n",
      "train epoch 3 loss : 534.013\n",
      "train epoch 3 loss : 511.159\n",
      "train epoch 3 loss : 510.589\n",
      "train epoch 3 loss : 497.079\n",
      "train epoch 3 loss : 351.339\n",
      "train epoch 3 loss : 498.050\n",
      "train epoch 3 loss : 375.019\n",
      "train epoch 3 loss : 370.174\n",
      "train epoch 3 loss : 340.830\n",
      "train epoch 3 loss : 383.191\n",
      "train epoch 3 loss : 301.798\n",
      "train epoch 3 loss : 345.249\n",
      "train epoch 3 loss : 290.909\n",
      "train epoch 3 loss : 245.173\n",
      "train epoch 3 loss : 262.960\n",
      "train epoch 3 loss : 203.206\n",
      "train epoch 4 loss : 19.614\n",
      "train epoch 4 loss : 172.967\n",
      "train epoch 4 loss : 234.755\n",
      "train epoch 4 loss : 148.888\n",
      "train epoch 4 loss : 212.788\n",
      "train epoch 4 loss : 107.891\n",
      "train epoch 4 loss : 126.282\n",
      "train epoch 4 loss : 122.992\n",
      "train epoch 4 loss : 135.965\n",
      "train epoch 4 loss : 156.229\n",
      "train epoch 4 loss : 102.880\n",
      "train epoch 4 loss : 146.751\n",
      "train epoch 4 loss : 99.522\n",
      "train epoch 4 loss : 98.573\n",
      "train epoch 4 loss : 88.487\n",
      "train epoch 4 loss : 93.486\n",
      "train epoch 4 loss : 105.324\n",
      "train epoch 4 loss : 93.289\n",
      "train epoch 4 loss : 91.980\n",
      "train epoch 4 loss : 74.361\n",
      "train epoch 4 loss : 77.257\n",
      "train epoch 4 loss : 74.648\n",
      "train epoch 4 loss : 69.277\n",
      "train epoch 4 loss : 59.972\n",
      "train epoch 4 loss : 61.966\n",
      "train epoch 5 loss : 4.423\n",
      "train epoch 5 loss : 55.982\n",
      "train epoch 5 loss : 64.879\n",
      "train epoch 5 loss : 51.570\n",
      "train epoch 5 loss : 61.524\n",
      "train epoch 5 loss : 49.443\n",
      "train epoch 5 loss : 47.186\n",
      "train epoch 5 loss : 47.393\n",
      "train epoch 5 loss : 51.042\n",
      "train epoch 5 loss : 54.090\n",
      "train epoch 5 loss : 51.510\n",
      "train epoch 5 loss : 42.621\n",
      "train epoch 5 loss : 66.973\n",
      "train epoch 5 loss : 43.226\n",
      "train epoch 5 loss : 44.349\n",
      "train epoch 5 loss : 42.137\n",
      "train epoch 5 loss : 45.982\n",
      "train epoch 5 loss : 48.056\n",
      "train epoch 5 loss : 53.521\n",
      "train epoch 5 loss : 41.170\n",
      "train epoch 5 loss : 57.060\n",
      "train epoch 5 loss : 44.891\n",
      "train epoch 5 loss : 42.008\n",
      "train epoch 5 loss : 46.063\n",
      "train epoch 5 loss : 43.800\n",
      "train epoch 6 loss : 2.605\n",
      "train epoch 6 loss : 47.962\n",
      "train epoch 6 loss : 48.654\n",
      "train epoch 6 loss : 41.121\n",
      "train epoch 6 loss : 39.030\n",
      "train epoch 6 loss : 47.869\n",
      "train epoch 6 loss : 41.796\n",
      "train epoch 6 loss : 34.846\n",
      "train epoch 6 loss : 45.002\n",
      "train epoch 6 loss : 39.650\n",
      "train epoch 6 loss : 35.809\n",
      "train epoch 6 loss : 45.572\n",
      "train epoch 6 loss : 30.666\n",
      "train epoch 6 loss : 43.051\n",
      "train epoch 6 loss : 40.477\n",
      "train epoch 6 loss : 40.415\n",
      "train epoch 6 loss : 39.696\n",
      "train epoch 6 loss : 36.660\n",
      "train epoch 6 loss : 38.594\n",
      "train epoch 6 loss : 34.327\n",
      "train epoch 6 loss : 42.816\n",
      "train epoch 6 loss : 39.952\n",
      "train epoch 6 loss : 34.058\n",
      "train epoch 6 loss : 34.943\n",
      "train epoch 6 loss : 40.066\n",
      "train epoch 7 loss : 3.484\n",
      "train epoch 7 loss : 36.403\n",
      "train epoch 7 loss : 35.097\n",
      "train epoch 7 loss : 46.628\n",
      "train epoch 7 loss : 37.280\n",
      "train epoch 7 loss : 40.665\n",
      "train epoch 7 loss : 40.440\n",
      "train epoch 7 loss : 33.856\n",
      "train epoch 7 loss : 36.340\n",
      "train epoch 7 loss : 35.266\n",
      "train epoch 7 loss : 36.133\n",
      "train epoch 7 loss : 34.975\n",
      "train epoch 7 loss : 34.420\n",
      "train epoch 7 loss : 31.155\n",
      "train epoch 7 loss : 31.788\n",
      "train epoch 7 loss : 39.960\n",
      "train epoch 7 loss : 40.409\n",
      "train epoch 7 loss : 36.559\n",
      "train epoch 7 loss : 36.404\n",
      "train epoch 7 loss : 33.870\n",
      "train epoch 7 loss : 33.388\n",
      "train epoch 7 loss : 37.308\n",
      "train epoch 7 loss : 35.241\n",
      "train epoch 7 loss : 31.919\n",
      "train epoch 7 loss : 36.974\n",
      "train epoch 8 loss : 4.107\n",
      "train epoch 8 loss : 36.220\n",
      "train epoch 8 loss : 34.470\n",
      "train epoch 8 loss : 28.870\n",
      "train epoch 8 loss : 40.431\n",
      "train epoch 8 loss : 38.536\n",
      "train epoch 8 loss : 33.680\n",
      "train epoch 8 loss : 27.659\n",
      "train epoch 8 loss : 28.621\n",
      "train epoch 8 loss : 30.494\n",
      "train epoch 8 loss : 35.326\n",
      "train epoch 8 loss : 25.018\n",
      "train epoch 8 loss : 33.455\n",
      "train epoch 8 loss : 32.912\n",
      "train epoch 8 loss : 32.585\n",
      "train epoch 8 loss : 33.482\n",
      "train epoch 8 loss : 26.393\n",
      "train epoch 8 loss : 26.497\n",
      "train epoch 8 loss : 38.483\n",
      "train epoch 8 loss : 28.262\n",
      "train epoch 8 loss : 35.036\n",
      "train epoch 8 loss : 33.035\n",
      "train epoch 8 loss : 27.598\n",
      "train epoch 8 loss : 28.626\n",
      "train epoch 8 loss : 31.192\n",
      "train epoch 9 loss : 4.255\n",
      "train epoch 9 loss : 29.978\n",
      "train epoch 9 loss : 32.043\n",
      "train epoch 9 loss : 24.524\n",
      "train epoch 9 loss : 27.620\n",
      "train epoch 9 loss : 29.469\n",
      "train epoch 9 loss : 24.744\n",
      "train epoch 9 loss : 26.250\n",
      "train epoch 9 loss : 27.929\n",
      "train epoch 9 loss : 27.060\n",
      "train epoch 9 loss : 27.271\n",
      "train epoch 9 loss : 27.653\n",
      "train epoch 9 loss : 27.568\n",
      "train epoch 9 loss : 30.624\n",
      "train epoch 9 loss : 30.821\n",
      "train epoch 9 loss : 31.983\n",
      "train epoch 9 loss : 26.748\n",
      "train epoch 9 loss : 24.009\n",
      "train epoch 9 loss : 29.305\n",
      "train epoch 9 loss : 25.083\n",
      "train epoch 9 loss : 25.839\n",
      "train epoch 9 loss : 24.311\n",
      "train epoch 9 loss : 22.873\n",
      "train epoch 9 loss : 30.325\n",
      "train epoch 9 loss : 29.215\n",
      "train epoch 10 loss : 2.579\n",
      "train epoch 10 loss : 25.154\n",
      "train epoch 10 loss : 24.466\n",
      "train epoch 10 loss : 17.125\n",
      "train epoch 10 loss : 21.832\n",
      "train epoch 10 loss : 22.133\n",
      "train epoch 10 loss : 30.734\n",
      "train epoch 10 loss : 27.267\n",
      "train epoch 10 loss : 22.378\n",
      "train epoch 10 loss : 21.407\n",
      "train epoch 10 loss : 22.811\n",
      "train epoch 10 loss : 22.584\n",
      "train epoch 10 loss : 24.391\n",
      "train epoch 10 loss : 21.734\n",
      "train epoch 10 loss : 22.504\n",
      "train epoch 10 loss : 20.530\n",
      "train epoch 10 loss : 21.258\n",
      "train epoch 10 loss : 20.575\n",
      "train epoch 10 loss : 19.329\n",
      "train epoch 10 loss : 21.443\n",
      "train epoch 10 loss : 20.437\n",
      "train epoch 10 loss : 20.249\n",
      "train epoch 10 loss : 16.304\n",
      "train epoch 10 loss : 20.286\n",
      "train epoch 10 loss : 18.879\n",
      "train epoch 11 loss : 1.551\n",
      "train epoch 11 loss : 22.920\n",
      "train epoch 11 loss : 18.109\n",
      "train epoch 11 loss : 16.943\n",
      "train epoch 11 loss : 17.693\n",
      "train epoch 11 loss : 18.370\n",
      "train epoch 11 loss : 19.088\n",
      "train epoch 11 loss : 14.201\n",
      "train epoch 11 loss : 14.828\n",
      "train epoch 11 loss : 20.022\n",
      "train epoch 11 loss : 19.757\n",
      "train epoch 11 loss : 13.999\n",
      "train epoch 11 loss : 15.122\n",
      "train epoch 11 loss : 20.263\n",
      "train epoch 11 loss : 16.852\n",
      "train epoch 11 loss : 17.773\n",
      "train epoch 11 loss : 14.501\n",
      "train epoch 11 loss : 14.629\n",
      "train epoch 11 loss : 12.580\n",
      "train epoch 11 loss : 15.669\n",
      "train epoch 11 loss : 13.383\n",
      "train epoch 11 loss : 12.065\n",
      "train epoch 11 loss : 13.362\n",
      "train epoch 11 loss : 14.269\n",
      "train epoch 11 loss : 12.292\n",
      "train epoch 12 loss : 1.711\n",
      "train epoch 12 loss : 15.173\n",
      "train epoch 12 loss : 13.074\n",
      "train epoch 12 loss : 10.711\n",
      "train epoch 12 loss : 13.408\n",
      "train epoch 12 loss : 13.472\n",
      "train epoch 12 loss : 12.181\n",
      "train epoch 12 loss : 10.741\n",
      "train epoch 12 loss : 9.225\n",
      "train epoch 12 loss : 10.928\n",
      "train epoch 12 loss : 11.792\n",
      "train epoch 12 loss : 7.300\n",
      "train epoch 12 loss : 9.941\n",
      "train epoch 12 loss : 10.542\n",
      "train epoch 12 loss : 10.438\n",
      "train epoch 12 loss : 9.828\n",
      "train epoch 12 loss : 9.187\n",
      "train epoch 12 loss : 7.872\n",
      "train epoch 12 loss : 8.461\n",
      "train epoch 12 loss : 7.725\n",
      "train epoch 12 loss : 9.206\n",
      "train epoch 12 loss : 8.245\n",
      "train epoch 12 loss : 9.963\n",
      "train epoch 12 loss : 7.654\n",
      "train epoch 12 loss : 8.058\n",
      "train epoch 13 loss : 1.255\n",
      "train epoch 13 loss : 5.646\n",
      "train epoch 13 loss : 6.066\n",
      "train epoch 13 loss : 6.816\n",
      "train epoch 13 loss : 5.576\n",
      "train epoch 13 loss : 6.633\n",
      "train epoch 13 loss : 6.099\n",
      "train epoch 13 loss : 7.297\n",
      "train epoch 13 loss : 6.849\n",
      "train epoch 13 loss : 6.219\n",
      "train epoch 13 loss : 6.160\n",
      "train epoch 13 loss : 6.096\n",
      "train epoch 13 loss : 5.554\n",
      "train epoch 13 loss : 5.177\n",
      "train epoch 13 loss : 4.868\n",
      "train epoch 13 loss : 7.304\n",
      "train epoch 13 loss : 6.878\n",
      "train epoch 13 loss : 6.542\n",
      "train epoch 13 loss : 4.423\n",
      "train epoch 13 loss : 5.328\n",
      "train epoch 13 loss : 5.233\n",
      "train epoch 13 loss : 5.291\n",
      "train epoch 13 loss : 4.955\n",
      "train epoch 13 loss : 4.309\n",
      "train epoch 13 loss : 5.186\n",
      "train epoch 14 loss : 0.462\n",
      "train epoch 14 loss : 3.994\n",
      "train epoch 14 loss : 5.017\n",
      "train epoch 14 loss : 3.849\n",
      "train epoch 14 loss : 3.861\n",
      "train epoch 14 loss : 3.189\n",
      "train epoch 14 loss : 4.494\n",
      "train epoch 14 loss : 4.026\n",
      "train epoch 14 loss : 4.037\n",
      "train epoch 14 loss : 3.635\n",
      "train epoch 14 loss : 3.948\n",
      "train epoch 14 loss : 4.199\n",
      "train epoch 14 loss : 3.862\n",
      "train epoch 14 loss : 3.708\n",
      "train epoch 14 loss : 3.390\n",
      "train epoch 14 loss : 4.224\n",
      "train epoch 14 loss : 2.989\n",
      "train epoch 14 loss : 2.785\n",
      "train epoch 14 loss : 3.864\n",
      "train epoch 14 loss : 3.545\n",
      "train epoch 14 loss : 3.402\n",
      "train epoch 14 loss : 3.916\n",
      "train epoch 14 loss : 3.534\n",
      "train epoch 14 loss : 2.861\n",
      "train epoch 14 loss : 3.053\n",
      "train epoch 15 loss : 0.280\n",
      "train epoch 15 loss : 3.111\n",
      "train epoch 15 loss : 2.805\n",
      "train epoch 15 loss : 3.674\n",
      "train epoch 15 loss : 3.375\n",
      "train epoch 15 loss : 3.296\n",
      "train epoch 15 loss : 3.438\n",
      "train epoch 15 loss : 2.814\n",
      "train epoch 15 loss : 2.995\n",
      "train epoch 15 loss : 2.812\n",
      "train epoch 15 loss : 3.255\n",
      "train epoch 15 loss : 3.423\n",
      "train epoch 15 loss : 2.809\n",
      "train epoch 15 loss : 2.635\n",
      "train epoch 15 loss : 2.621\n",
      "train epoch 15 loss : 2.894\n",
      "train epoch 15 loss : 2.702\n",
      "train epoch 15 loss : 3.124\n",
      "train epoch 15 loss : 5.751\n",
      "train epoch 15 loss : 4.443\n",
      "train epoch 15 loss : 3.584\n",
      "train epoch 15 loss : 3.330\n",
      "train epoch 15 loss : 3.873\n",
      "train epoch 15 loss : 2.420\n",
      "train epoch 15 loss : 2.623\n",
      "train epoch 16 loss : 0.259\n",
      "train epoch 16 loss : 2.558\n",
      "train epoch 16 loss : 2.663\n",
      "train epoch 16 loss : 2.546\n",
      "train epoch 16 loss : 2.507\n",
      "train epoch 16 loss : 2.064\n",
      "train epoch 16 loss : 3.149\n",
      "train epoch 16 loss : 2.826\n",
      "train epoch 16 loss : 2.620\n",
      "train epoch 16 loss : 2.940\n",
      "train epoch 16 loss : 2.328\n",
      "train epoch 16 loss : 2.584\n",
      "train epoch 16 loss : 2.470\n",
      "train epoch 16 loss : 1.907\n",
      "train epoch 16 loss : 2.621\n",
      "train epoch 16 loss : 2.106\n",
      "train epoch 16 loss : 2.120\n",
      "train epoch 16 loss : 2.079\n",
      "train epoch 16 loss : 2.396\n",
      "train epoch 16 loss : 2.886\n",
      "train epoch 16 loss : 2.698\n",
      "train epoch 16 loss : 2.829\n",
      "train epoch 16 loss : 1.970\n",
      "train epoch 16 loss : 2.325\n",
      "train epoch 16 loss : 1.670\n",
      "train epoch 17 loss : 0.252\n",
      "train epoch 17 loss : 1.984\n",
      "train epoch 17 loss : 2.212\n",
      "train epoch 17 loss : 1.979\n",
      "train epoch 17 loss : 2.099\n",
      "train epoch 17 loss : 1.712\n",
      "train epoch 17 loss : 2.180\n",
      "train epoch 17 loss : 1.603\n",
      "train epoch 17 loss : 1.684\n",
      "train epoch 17 loss : 1.675\n",
      "train epoch 17 loss : 2.109\n",
      "train epoch 17 loss : 2.424\n",
      "train epoch 17 loss : 1.790\n",
      "train epoch 17 loss : 1.787\n",
      "train epoch 17 loss : 2.405\n",
      "train epoch 17 loss : 1.908\n",
      "train epoch 17 loss : 1.997\n",
      "train epoch 17 loss : 1.589\n",
      "train epoch 17 loss : 2.265\n",
      "train epoch 17 loss : 1.991\n",
      "train epoch 17 loss : 1.784\n",
      "train epoch 17 loss : 1.467\n",
      "train epoch 17 loss : 1.880\n",
      "train epoch 17 loss : 1.827\n",
      "train epoch 17 loss : 1.767\n",
      "train epoch 18 loss : 0.141\n",
      "train epoch 18 loss : 1.567\n",
      "train epoch 18 loss : 1.810\n",
      "train epoch 18 loss : 1.579\n",
      "train epoch 18 loss : 1.544\n",
      "train epoch 18 loss : 1.711\n",
      "train epoch 18 loss : 1.464\n",
      "train epoch 18 loss : 1.988\n",
      "train epoch 18 loss : 2.305\n",
      "train epoch 18 loss : 1.877\n",
      "train epoch 18 loss : 1.703\n",
      "train epoch 18 loss : 1.866\n",
      "train epoch 18 loss : 1.399\n",
      "train epoch 18 loss : 1.368\n",
      "train epoch 18 loss : 1.524\n",
      "train epoch 18 loss : 1.872\n",
      "train epoch 18 loss : 1.516\n",
      "train epoch 18 loss : 2.166\n",
      "train epoch 18 loss : 1.580\n",
      "train epoch 18 loss : 1.352\n",
      "train epoch 18 loss : 1.304\n",
      "train epoch 18 loss : 1.709\n",
      "train epoch 18 loss : 1.326\n",
      "train epoch 18 loss : 1.382\n",
      "train epoch 18 loss : 1.453\n",
      "train epoch 19 loss : 0.105\n",
      "train epoch 19 loss : 1.532\n",
      "train epoch 19 loss : 1.414\n",
      "train epoch 19 loss : 1.354\n",
      "train epoch 19 loss : 1.660\n",
      "train epoch 19 loss : 1.239\n",
      "train epoch 19 loss : 1.164\n",
      "train epoch 19 loss : 1.263\n",
      "train epoch 19 loss : 1.094\n",
      "train epoch 19 loss : 1.497\n",
      "train epoch 19 loss : 1.245\n",
      "train epoch 19 loss : 1.700\n",
      "train epoch 19 loss : 1.242\n",
      "train epoch 19 loss : 1.189\n",
      "train epoch 19 loss : 1.641\n",
      "train epoch 19 loss : 1.334\n",
      "train epoch 19 loss : 1.929\n",
      "train epoch 19 loss : 1.951\n",
      "train epoch 19 loss : 1.847\n",
      "train epoch 19 loss : 1.678\n",
      "train epoch 19 loss : 1.933\n",
      "train epoch 19 loss : 2.162\n",
      "train epoch 19 loss : 1.888\n",
      "train epoch 19 loss : 1.799\n",
      "train epoch 19 loss : 1.595\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr =0.001)\n",
    "criteon= nn.MSELoss()\n",
    "EPOCH = 20\n",
    "loss_list = []\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()\n",
    "    sum_loss = 0 \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        #data, target = Variable(data), Variable(target)\n",
    "\n",
    "        data = torch.reshape(data,(batchsize, 1, 16))\n",
    "        target = torch.reshape(target, (batchsize, 1))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        data=data.to(torch.float32)\n",
    "        target=target.to(torch.float32)\n",
    "        output = model(data)\n",
    "\n",
    "        \n",
    "        #print(data)\n",
    "        #print(target)\n",
    "        #print(\"The output is:\")\n",
    "        #print(output)\n",
    "        \n",
    "        loss = criteon(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\"train epoch {} loss : {:.3f}\".format(epoch, sum_loss/100))\n",
    "            loss_list.append(sum_loss/100)\n",
    "            sum_loss = 0         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364.27740478515625"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.cpu()\n",
    "model(torch.Tensor(X_test[0].reshape(1,1,16))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test数据集上的表现\n",
    "testing = pd.DataFrame(X_test)\n",
    "#testing['truth'] = Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = model(torch.Tensor(X_test.reshape(2000,1,16))).detach().numpy()\n",
    "re = re.reshape(2000)\n",
    "true = Y_test.numpy()\n",
    "dataframe = pd.DataFrame([re, true]).T\n",
    "dataframe.columns = ['pred','truth']\n",
    "dataframe['error'] = np.abs(dataframe['pred'] - dataframe['truth'])\n",
    "dataframe['percentage'] = dataframe['error']/np.abs(dataframe['truth'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.141919788293053"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['percentage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>truth</th>\n",
       "      <th>error</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>364.277435</td>\n",
       "      <td>362.790985</td>\n",
       "      <td>1.486450</td>\n",
       "      <td>0.004097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.721594</td>\n",
       "      <td>0.204353</td>\n",
       "      <td>2.517241</td>\n",
       "      <td>12.318089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.880503</td>\n",
       "      <td>0.775139</td>\n",
       "      <td>2.105364</td>\n",
       "      <td>2.716111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>446.227753</td>\n",
       "      <td>448.795654</td>\n",
       "      <td>2.567902</td>\n",
       "      <td>0.005722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>301.720673</td>\n",
       "      <td>299.815735</td>\n",
       "      <td>1.904938</td>\n",
       "      <td>0.006354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>409.090240</td>\n",
       "      <td>407.261139</td>\n",
       "      <td>1.829102</td>\n",
       "      <td>0.004491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>106.116310</td>\n",
       "      <td>107.372185</td>\n",
       "      <td>1.255875</td>\n",
       "      <td>0.011696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>15.442434</td>\n",
       "      <td>15.047541</td>\n",
       "      <td>0.394894</td>\n",
       "      <td>0.026243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>242.997787</td>\n",
       "      <td>239.743607</td>\n",
       "      <td>3.254181</td>\n",
       "      <td>0.013574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>80.363251</td>\n",
       "      <td>81.134918</td>\n",
       "      <td>0.771667</td>\n",
       "      <td>0.009511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pred       truth     error  percentage\n",
       "0     364.277435  362.790985  1.486450    0.004097\n",
       "1       2.721594    0.204353  2.517241   12.318089\n",
       "2       2.880503    0.775139  2.105364    2.716111\n",
       "3     446.227753  448.795654  2.567902    0.005722\n",
       "4     301.720673  299.815735  1.904938    0.006354\n",
       "...          ...         ...       ...         ...\n",
       "1995  409.090240  407.261139  1.829102    0.004491\n",
       "1996  106.116310  107.372185  1.255875    0.011696\n",
       "1997   15.442434   15.047541  0.394894    0.026243\n",
       "1998  242.997787  239.743607  3.254181    0.013574\n",
       "1999   80.363251   81.134918  0.771667    0.009511\n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
