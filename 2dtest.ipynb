{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch import nn,optim\n",
    "from torch.nn import functional as F \n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = np.load('2ddata.npy')\n",
    "trainlabel = np.load('2dlabel.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 43.52606322, -13.99713994,   9.49569717, ..., -12.22158433,\n",
       "         6.01721102, -24.02406788])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(traindata, trainlabel, test_size = 0.2, random_state = 1)\n",
    "X_train, Y_train = torch.FloatTensor(X_train), torch.FloatTensor(Y_train)\n",
    "X_test, Y_test = torch.FloatTensor(X_test), torch.FloatTensor(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 8\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "\n",
    "#dataloader \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batchsize, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batchsize, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16000, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_16d2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_16d2,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, 2,  stride=2)\n",
    "        self.conv2 = nn.Conv2d(4, 16, 2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(16, 160, 2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(160, 80, 2, stride=2)\n",
    "        self.conv5 = nn.Conv2d(80, 20, 2, stride=2)\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(20, 60)\n",
    "        self.fc2 = nn.Linear(60, 60)\n",
    "        self.fc3 = nn.Linear(60, 20)\n",
    "        self.fc4 = nn.Linear(20, 1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv4(x)  \n",
    "        #print(x.shape)\n",
    "        x = self.conv5(x)  \n",
    "        #print(x.shape)\n",
    "\n",
    "        x = self.flat(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0230]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(1,1,32,32)\n",
    "xiao = CNN_16d2()\n",
    "xiao(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_available\n"
     ]
    }
   ],
   "source": [
    "model = CNN_16d2()\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu_available\")\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 0 loss : 1252.458 \n",
      "train epoch 0 loss : 1211.436 \n",
      "train epoch 0 loss : 1628.970 \n",
      "train epoch 0 loss : 2255.892 \n",
      "train epoch 0 loss : 2212.110 \n",
      "train epoch 0 loss : 2488.308 \n",
      "train epoch 0 loss : 1174.659 \n",
      "train epoch 0 loss : 1566.575 \n",
      "train epoch 0 loss : 1287.551 \n",
      "train epoch 0 loss : 1673.098 \n",
      "train epoch 0 loss : 1559.372 \n",
      "train epoch 0 loss : 1128.815 \n",
      "train epoch 0 loss : 1483.951 \n",
      "train epoch 0 loss : 1310.568 \n",
      "train epoch 0 loss : 1177.141 \n",
      "train epoch 0 loss : 1125.729 \n",
      "train epoch 0 loss : 1442.870 \n",
      "train epoch 0 loss : 1510.841 \n",
      "train epoch 0 loss : 1704.030 \n",
      "train epoch 0 loss : 1365.248 \n",
      "train epoch 0 loss : 1499.358 \n",
      "train epoch 0 loss : 1601.261 \n",
      "train epoch 0 loss : 1569.443 \n",
      "train epoch 0 loss : 1811.768 \n",
      "train epoch 0 loss : 1483.493 \n",
      "train epoch 0 loss : 1312.738 \n",
      "train epoch 0 loss : 1018.830 \n",
      "train epoch 0 loss : 1555.806 \n",
      "train epoch 0 loss : 1308.251 \n",
      "train epoch 0 loss : 1179.168 \n",
      "train epoch 0 loss : 1473.892 \n",
      "train epoch 0 loss : 1655.182 \n",
      "train epoch 0 loss : 1672.199 \n",
      "train epoch 0 loss : 1289.272 \n",
      "train epoch 0 loss : 1555.871 \n",
      "train epoch 0 loss : 1068.654 \n",
      "train epoch 0 loss : 1148.838 \n",
      "train epoch 0 loss : 1366.323 \n",
      "train epoch 0 loss : 1167.466 \n",
      "train epoch 0 loss : 774.349 \n",
      "train epoch 0 loss : 474.445 \n",
      "train epoch 0 loss : 622.723 \n",
      "train epoch 0 loss : 438.022 \n",
      "train epoch 0 loss : 453.898 \n",
      "train epoch 0 loss : 631.590 \n",
      "train epoch 0 loss : 473.617 \n",
      "train epoch 0 loss : 395.656 \n",
      "train epoch 0 loss : 420.827 \n",
      "train epoch 0 loss : 373.866 \n",
      "train epoch 0 loss : 492.214 \n",
      "train epoch 0 loss : 461.948 \n",
      "train epoch 0 loss : 373.853 \n",
      "train epoch 0 loss : 359.225 \n",
      "train epoch 0 loss : 422.966 \n",
      "train epoch 0 loss : 435.857 \n",
      "train epoch 0 loss : 363.986 \n",
      "train epoch 0 loss : 486.314 \n",
      "train epoch 0 loss : 353.683 \n",
      "train epoch 0 loss : 349.102 \n",
      "train epoch 0 loss : 349.541 \n",
      "train epoch 0 loss : 382.441 \n",
      "train epoch 0 loss : 282.971 \n",
      "train epoch 0 loss : 202.485 \n",
      "train epoch 0 loss : 178.958 \n",
      "train epoch 0 loss : 243.398 \n",
      "train epoch 0 loss : 180.594 \n",
      "train epoch 0 loss : 126.201 \n",
      "train epoch 0 loss : 163.208 \n",
      "train epoch 0 loss : 140.746 \n",
      "train epoch 0 loss : 174.726 \n",
      "train epoch 0 loss : 178.441 \n",
      "train epoch 0 loss : 265.592 \n",
      "train epoch 0 loss : 178.962 \n",
      "train epoch 0 loss : 182.171 \n",
      "train epoch 0 loss : 225.565 \n",
      "train epoch 0 loss : 118.137 \n",
      "train epoch 0 loss : 162.089 \n",
      "train epoch 0 loss : 109.585 \n",
      "train epoch 0 loss : 72.632 \n",
      "train epoch 0 loss : 129.673 \n",
      "train epoch 0 loss : 176.746 \n",
      "train epoch 0 loss : 165.241 \n",
      "train epoch 0 loss : 168.250 \n",
      "train epoch 0 loss : 132.258 \n",
      "train epoch 0 loss : 195.049 \n",
      "train epoch 0 loss : 160.447 \n",
      "train epoch 0 loss : 154.626 \n",
      "train epoch 0 loss : 135.852 \n",
      "train epoch 0 loss : 171.825 \n",
      "train epoch 0 loss : 153.233 \n",
      "train epoch 0 loss : 93.533 \n",
      "train epoch 0 loss : 159.568 \n",
      "train epoch 0 loss : 177.132 \n",
      "train epoch 0 loss : 183.428 \n",
      "train epoch 0 loss : 150.314 \n",
      "train epoch 0 loss : 193.813 \n",
      "train epoch 0 loss : 177.289 \n",
      "train epoch 0 loss : 148.194 \n",
      "train epoch 0 loss : 142.965 \n",
      "train epoch 0 loss : 141.305 \n",
      "train epoch 0 loss : 203.242 \n",
      "train epoch 0 loss : 139.913 \n",
      "train epoch 0 loss : 175.121 \n",
      "train epoch 0 loss : 148.026 \n",
      "train epoch 0 loss : 176.577 \n",
      "train epoch 0 loss : 147.640 \n",
      "train epoch 0 loss : 132.224 \n",
      "train epoch 0 loss : 223.408 \n",
      "train epoch 0 loss : 133.227 \n",
      "train epoch 0 loss : 119.483 \n",
      "train epoch 0 loss : 123.671 \n",
      "train epoch 0 loss : 157.024 \n",
      "train epoch 0 loss : 118.563 \n",
      "train epoch 0 loss : 113.204 \n",
      "train epoch 0 loss : 118.605 \n",
      "train epoch 0 loss : 128.329 \n",
      "train epoch 0 loss : 118.793 \n",
      "train epoch 0 loss : 155.784 \n",
      "train epoch 0 loss : 211.194 \n",
      "train epoch 0 loss : 138.627 \n",
      "train epoch 0 loss : 113.413 \n",
      "train epoch 0 loss : 142.810 \n",
      "train epoch 0 loss : 174.457 \n",
      "train epoch 0 loss : 157.962 \n",
      "train epoch 0 loss : 185.872 \n",
      "train epoch 0 loss : 169.707 \n",
      "train epoch 0 loss : 172.679 \n",
      "train epoch 0 loss : 239.193 \n",
      "train epoch 0 loss : 105.028 \n",
      "train epoch 0 loss : 91.982 \n",
      "train epoch 0 loss : 122.123 \n",
      "train epoch 0 loss : 137.491 \n",
      "train epoch 0 loss : 103.181 \n",
      "train epoch 0 loss : 146.702 \n",
      "train epoch 0 loss : 100.005 \n",
      "train epoch 0 loss : 169.487 \n",
      "train epoch 0 loss : 124.858 \n",
      "train epoch 0 loss : 207.220 \n",
      "train epoch 0 loss : 152.193 \n",
      "train epoch 0 loss : 101.396 \n",
      "train epoch 0 loss : 77.651 \n",
      "train epoch 0 loss : 105.844 \n",
      "train epoch 0 loss : 92.170 \n",
      "train epoch 0 loss : 108.974 \n",
      "train epoch 0 loss : 117.179 \n",
      "train epoch 0 loss : 101.159 \n",
      "train epoch 0 loss : 78.332 \n",
      "train epoch 0 loss : 102.715 \n",
      "train epoch 0 loss : 102.567 \n",
      "train epoch 0 loss : 126.444 \n",
      "train epoch 0 loss : 87.227 \n",
      "train epoch 0 loss : 114.384 \n",
      "train epoch 0 loss : 110.454 \n",
      "train epoch 0 loss : 169.522 \n",
      "train epoch 0 loss : 124.471 \n",
      "train epoch 0 loss : 96.576 \n",
      "train epoch 0 loss : 124.996 \n",
      "train epoch 0 loss : 109.346 \n",
      "train epoch 0 loss : 77.823 \n",
      "train epoch 0 loss : 102.322 \n",
      "train epoch 0 loss : 135.864 \n",
      "train epoch 0 loss : 87.919 \n",
      "train epoch 0 loss : 89.335 \n",
      "train epoch 0 loss : 130.019 \n",
      "train epoch 0 loss : 124.380 \n",
      "train epoch 0 loss : 114.390 \n",
      "train epoch 0 loss : 113.281 \n",
      "train epoch 0 loss : 80.828 \n",
      "train epoch 0 loss : 88.803 \n",
      "train epoch 0 loss : 106.228 \n",
      "train epoch 0 loss : 62.497 \n",
      "train epoch 0 loss : 74.705 \n",
      "train epoch 0 loss : 66.090 \n",
      "train epoch 0 loss : 65.040 \n",
      "train epoch 0 loss : 86.556 \n",
      "train epoch 0 loss : 82.168 \n",
      "train epoch 0 loss : 107.020 \n",
      "train epoch 0 loss : 105.993 \n",
      "train epoch 0 loss : 106.263 \n",
      "train epoch 0 loss : 101.825 \n",
      "train epoch 0 loss : 142.384 \n",
      "train epoch 0 loss : 89.737 \n",
      "train epoch 0 loss : 59.394 \n",
      "train epoch 0 loss : 102.004 \n",
      "train epoch 0 loss : 136.691 \n",
      "train epoch 0 loss : 111.563 \n",
      "train epoch 0 loss : 72.357 \n",
      "train epoch 0 loss : 103.485 \n",
      "train epoch 0 loss : 84.973 \n",
      "train epoch 0 loss : 104.078 \n",
      "train epoch 0 loss : 133.098 \n",
      "train epoch 0 loss : 97.446 \n",
      "train epoch 0 loss : 93.390 \n",
      "train epoch 0 loss : 121.109 \n",
      "train epoch 0 loss : 100.236 \n",
      "train epoch 0 loss : 76.918 \n",
      "train epoch 0 loss : 122.843 \n",
      "train epoch 0 loss : 121.385 \n",
      "train epoch 0 loss : 74.098 \n",
      "train epoch 0 loss : 80.676 \n",
      "train epoch 1 loss : 55.083 \n",
      "train epoch 1 loss : 140.259 \n",
      "train epoch 1 loss : 109.045 \n",
      "train epoch 1 loss : 83.032 \n",
      "train epoch 1 loss : 107.631 \n",
      "train epoch 1 loss : 79.728 \n",
      "train epoch 1 loss : 59.521 \n",
      "train epoch 1 loss : 103.129 \n",
      "train epoch 1 loss : 67.321 \n",
      "train epoch 1 loss : 110.698 \n",
      "train epoch 1 loss : 109.696 \n",
      "train epoch 1 loss : 94.045 \n",
      "train epoch 1 loss : 90.852 \n",
      "train epoch 1 loss : 85.602 \n",
      "train epoch 1 loss : 86.039 \n",
      "train epoch 1 loss : 65.418 \n",
      "train epoch 1 loss : 89.304 \n",
      "train epoch 1 loss : 86.895 \n",
      "train epoch 1 loss : 73.521 \n",
      "train epoch 1 loss : 64.586 \n",
      "train epoch 1 loss : 86.120 \n",
      "train epoch 1 loss : 105.284 \n",
      "train epoch 1 loss : 63.288 \n",
      "train epoch 1 loss : 89.984 \n",
      "train epoch 1 loss : 43.993 \n",
      "train epoch 1 loss : 141.080 \n",
      "train epoch 1 loss : 140.162 \n",
      "train epoch 1 loss : 73.952 \n",
      "train epoch 1 loss : 73.870 \n",
      "train epoch 1 loss : 71.997 \n",
      "train epoch 1 loss : 72.509 \n",
      "train epoch 1 loss : 107.309 \n",
      "train epoch 1 loss : 118.547 \n",
      "train epoch 1 loss : 84.294 \n",
      "train epoch 1 loss : 54.590 \n",
      "train epoch 1 loss : 85.033 \n",
      "train epoch 1 loss : 75.678 \n",
      "train epoch 1 loss : 83.628 \n",
      "train epoch 1 loss : 91.228 \n",
      "train epoch 1 loss : 117.110 \n",
      "train epoch 1 loss : 108.168 \n",
      "train epoch 1 loss : 143.422 \n",
      "train epoch 1 loss : 91.846 \n",
      "train epoch 1 loss : 63.617 \n",
      "train epoch 1 loss : 54.421 \n",
      "train epoch 1 loss : 61.643 \n",
      "train epoch 1 loss : 96.025 \n",
      "train epoch 1 loss : 104.184 \n",
      "train epoch 1 loss : 80.342 \n",
      "train epoch 1 loss : 75.750 \n",
      "train epoch 1 loss : 77.501 \n",
      "train epoch 1 loss : 94.889 \n",
      "train epoch 1 loss : 84.907 \n",
      "train epoch 1 loss : 106.577 \n",
      "train epoch 1 loss : 150.674 \n",
      "train epoch 1 loss : 94.041 \n",
      "train epoch 1 loss : 157.293 \n",
      "train epoch 1 loss : 107.804 \n",
      "train epoch 1 loss : 113.809 \n",
      "train epoch 1 loss : 99.597 \n",
      "train epoch 1 loss : 73.336 \n",
      "train epoch 1 loss : 84.230 \n",
      "train epoch 1 loss : 106.889 \n",
      "train epoch 1 loss : 91.609 \n",
      "train epoch 1 loss : 120.816 \n",
      "train epoch 1 loss : 114.055 \n",
      "train epoch 1 loss : 101.212 \n",
      "train epoch 1 loss : 124.329 \n",
      "train epoch 1 loss : 70.941 \n",
      "train epoch 1 loss : 108.453 \n",
      "train epoch 1 loss : 89.769 \n",
      "train epoch 1 loss : 139.565 \n",
      "train epoch 1 loss : 98.969 \n",
      "train epoch 1 loss : 110.089 \n",
      "train epoch 1 loss : 87.370 \n",
      "train epoch 1 loss : 75.998 \n",
      "train epoch 1 loss : 77.921 \n",
      "train epoch 1 loss : 90.354 \n",
      "train epoch 1 loss : 69.366 \n",
      "train epoch 1 loss : 71.478 \n",
      "train epoch 1 loss : 91.040 \n",
      "train epoch 1 loss : 88.688 \n",
      "train epoch 1 loss : 95.168 \n",
      "train epoch 1 loss : 74.474 \n",
      "train epoch 1 loss : 73.845 \n",
      "train epoch 1 loss : 57.073 \n",
      "train epoch 1 loss : 62.453 \n",
      "train epoch 1 loss : 55.913 \n",
      "train epoch 1 loss : 108.113 \n",
      "train epoch 1 loss : 86.291 \n",
      "train epoch 1 loss : 69.832 \n",
      "train epoch 1 loss : 79.114 \n",
      "train epoch 1 loss : 103.323 \n",
      "train epoch 1 loss : 82.135 \n",
      "train epoch 1 loss : 87.706 \n",
      "train epoch 1 loss : 93.928 \n",
      "train epoch 1 loss : 63.937 \n",
      "train epoch 1 loss : 87.613 \n",
      "train epoch 1 loss : 86.966 \n",
      "train epoch 1 loss : 105.871 \n",
      "train epoch 1 loss : 128.931 \n",
      "train epoch 1 loss : 119.447 \n",
      "train epoch 1 loss : 89.226 \n",
      "train epoch 1 loss : 87.996 \n",
      "train epoch 1 loss : 100.752 \n",
      "train epoch 1 loss : 75.818 \n",
      "train epoch 1 loss : 107.638 \n",
      "train epoch 1 loss : 86.337 \n",
      "train epoch 1 loss : 81.964 \n",
      "train epoch 1 loss : 59.979 \n",
      "train epoch 1 loss : 85.586 \n",
      "train epoch 1 loss : 97.017 \n",
      "train epoch 1 loss : 104.131 \n",
      "train epoch 1 loss : 97.418 \n",
      "train epoch 1 loss : 82.151 \n",
      "train epoch 1 loss : 82.502 \n",
      "train epoch 1 loss : 74.953 \n",
      "train epoch 1 loss : 68.432 \n",
      "train epoch 1 loss : 58.821 \n",
      "train epoch 1 loss : 110.969 \n",
      "train epoch 1 loss : 93.629 \n",
      "train epoch 1 loss : 132.572 \n",
      "train epoch 1 loss : 86.990 \n",
      "train epoch 1 loss : 82.968 \n",
      "train epoch 1 loss : 63.505 \n",
      "train epoch 1 loss : 100.492 \n",
      "train epoch 1 loss : 66.662 \n",
      "train epoch 1 loss : 49.447 \n",
      "train epoch 1 loss : 86.707 \n",
      "train epoch 1 loss : 72.150 \n",
      "train epoch 1 loss : 71.882 \n",
      "train epoch 1 loss : 95.464 \n",
      "train epoch 1 loss : 104.891 \n",
      "train epoch 1 loss : 68.208 \n",
      "train epoch 1 loss : 51.479 \n",
      "train epoch 1 loss : 66.528 \n",
      "train epoch 1 loss : 42.770 \n",
      "train epoch 1 loss : 82.132 \n",
      "train epoch 1 loss : 83.882 \n",
      "train epoch 1 loss : 53.177 \n",
      "train epoch 1 loss : 58.256 \n",
      "train epoch 1 loss : 82.314 \n",
      "train epoch 1 loss : 62.790 \n",
      "train epoch 1 loss : 49.345 \n",
      "train epoch 1 loss : 58.413 \n",
      "train epoch 1 loss : 74.985 \n",
      "train epoch 1 loss : 83.560 \n",
      "train epoch 1 loss : 88.267 \n",
      "train epoch 1 loss : 87.959 \n",
      "train epoch 1 loss : 65.388 \n",
      "train epoch 1 loss : 77.976 \n",
      "train epoch 1 loss : 60.229 \n",
      "train epoch 1 loss : 62.241 \n",
      "train epoch 1 loss : 63.009 \n",
      "train epoch 1 loss : 69.238 \n",
      "train epoch 1 loss : 118.434 \n",
      "train epoch 1 loss : 90.095 \n",
      "train epoch 1 loss : 113.657 \n",
      "train epoch 1 loss : 90.737 \n",
      "train epoch 1 loss : 64.997 \n",
      "train epoch 1 loss : 67.216 \n",
      "train epoch 1 loss : 67.262 \n",
      "train epoch 1 loss : 63.081 \n",
      "train epoch 1 loss : 39.314 \n",
      "train epoch 1 loss : 98.458 \n",
      "train epoch 1 loss : 126.183 \n",
      "train epoch 1 loss : 90.022 \n",
      "train epoch 1 loss : 99.926 \n",
      "train epoch 1 loss : 71.141 \n",
      "train epoch 1 loss : 77.838 \n",
      "train epoch 1 loss : 83.868 \n",
      "train epoch 1 loss : 87.199 \n",
      "train epoch 1 loss : 89.950 \n",
      "train epoch 1 loss : 114.938 \n",
      "train epoch 1 loss : 88.915 \n",
      "train epoch 1 loss : 67.983 \n",
      "train epoch 1 loss : 77.861 \n",
      "train epoch 1 loss : 81.355 \n",
      "train epoch 1 loss : 95.913 \n",
      "train epoch 1 loss : 61.615 \n",
      "train epoch 1 loss : 72.289 \n",
      "train epoch 1 loss : 77.253 \n",
      "train epoch 1 loss : 128.095 \n",
      "train epoch 1 loss : 63.363 \n",
      "train epoch 1 loss : 86.534 \n",
      "train epoch 1 loss : 84.738 \n",
      "train epoch 1 loss : 60.152 \n",
      "train epoch 1 loss : 60.304 \n",
      "train epoch 1 loss : 88.156 \n",
      "train epoch 1 loss : 76.961 \n",
      "train epoch 1 loss : 95.851 \n",
      "train epoch 1 loss : 110.972 \n",
      "train epoch 1 loss : 129.211 \n",
      "train epoch 1 loss : 99.568 \n",
      "train epoch 1 loss : 100.020 \n",
      "train epoch 1 loss : 71.423 \n",
      "train epoch 1 loss : 118.816 \n",
      "train epoch 1 loss : 66.050 \n",
      "train epoch 1 loss : 86.877 \n",
      "train epoch 1 loss : 113.003 \n",
      "train epoch 2 loss : 146.147 \n",
      "train epoch 2 loss : 131.303 \n",
      "train epoch 2 loss : 129.016 \n",
      "train epoch 2 loss : 65.827 \n",
      "train epoch 2 loss : 70.001 \n",
      "train epoch 2 loss : 48.751 \n",
      "train epoch 2 loss : 63.485 \n",
      "train epoch 2 loss : 39.012 \n",
      "train epoch 2 loss : 73.128 \n",
      "train epoch 2 loss : 70.764 \n",
      "train epoch 2 loss : 84.163 \n",
      "train epoch 2 loss : 53.944 \n",
      "train epoch 2 loss : 90.558 \n",
      "train epoch 2 loss : 81.667 \n",
      "train epoch 2 loss : 62.301 \n",
      "train epoch 2 loss : 55.622 \n",
      "train epoch 2 loss : 94.377 \n",
      "train epoch 2 loss : 62.669 \n",
      "train epoch 2 loss : 66.351 \n",
      "train epoch 2 loss : 118.419 \n",
      "train epoch 2 loss : 74.207 \n",
      "train epoch 2 loss : 76.172 \n",
      "train epoch 2 loss : 103.699 \n",
      "train epoch 2 loss : 61.225 \n",
      "train epoch 2 loss : 66.895 \n",
      "train epoch 2 loss : 99.328 \n",
      "train epoch 2 loss : 66.494 \n",
      "train epoch 2 loss : 95.470 \n",
      "train epoch 2 loss : 63.878 \n",
      "train epoch 2 loss : 69.237 \n",
      "train epoch 2 loss : 81.609 \n",
      "train epoch 2 loss : 93.426 \n",
      "train epoch 2 loss : 148.306 \n",
      "train epoch 2 loss : 61.402 \n",
      "train epoch 2 loss : 70.022 \n",
      "train epoch 2 loss : 118.681 \n",
      "train epoch 2 loss : 119.425 \n",
      "train epoch 2 loss : 97.310 \n",
      "train epoch 2 loss : 78.569 \n",
      "train epoch 2 loss : 61.254 \n",
      "train epoch 2 loss : 61.684 \n",
      "train epoch 2 loss : 78.670 \n",
      "train epoch 2 loss : 109.425 \n",
      "train epoch 2 loss : 85.224 \n",
      "train epoch 2 loss : 93.801 \n",
      "train epoch 2 loss : 54.286 \n",
      "train epoch 2 loss : 67.778 \n",
      "train epoch 2 loss : 82.367 \n",
      "train epoch 2 loss : 51.305 \n",
      "train epoch 2 loss : 78.402 \n",
      "train epoch 2 loss : 62.988 \n",
      "train epoch 2 loss : 67.013 \n",
      "train epoch 2 loss : 60.759 \n",
      "train epoch 2 loss : 71.103 \n",
      "train epoch 2 loss : 82.673 \n",
      "train epoch 2 loss : 80.605 \n",
      "train epoch 2 loss : 112.058 \n",
      "train epoch 2 loss : 45.995 \n",
      "train epoch 2 loss : 79.768 \n",
      "train epoch 2 loss : 63.319 \n",
      "train epoch 2 loss : 94.025 \n",
      "train epoch 2 loss : 65.982 \n",
      "train epoch 2 loss : 67.995 \n",
      "train epoch 2 loss : 68.780 \n",
      "train epoch 2 loss : 74.519 \n",
      "train epoch 2 loss : 58.250 \n",
      "train epoch 2 loss : 64.233 \n",
      "train epoch 2 loss : 75.027 \n",
      "train epoch 2 loss : 76.013 \n",
      "train epoch 2 loss : 57.197 \n",
      "train epoch 2 loss : 77.134 \n",
      "train epoch 2 loss : 80.946 \n",
      "train epoch 2 loss : 72.983 \n",
      "train epoch 2 loss : 72.953 \n",
      "train epoch 2 loss : 92.496 \n",
      "train epoch 2 loss : 92.782 \n",
      "train epoch 2 loss : 78.283 \n",
      "train epoch 2 loss : 48.080 \n",
      "train epoch 2 loss : 92.001 \n",
      "train epoch 2 loss : 76.359 \n",
      "train epoch 2 loss : 80.698 \n",
      "train epoch 2 loss : 50.082 \n",
      "train epoch 2 loss : 54.489 \n",
      "train epoch 2 loss : 76.658 \n",
      "train epoch 2 loss : 59.041 \n",
      "train epoch 2 loss : 63.654 \n",
      "train epoch 2 loss : 67.138 \n",
      "train epoch 2 loss : 53.896 \n",
      "train epoch 2 loss : 59.620 \n",
      "train epoch 2 loss : 49.190 \n",
      "train epoch 2 loss : 64.066 \n",
      "train epoch 2 loss : 67.124 \n",
      "train epoch 2 loss : 84.299 \n",
      "train epoch 2 loss : 84.108 \n",
      "train epoch 2 loss : 101.058 \n",
      "train epoch 2 loss : 102.310 \n",
      "train epoch 2 loss : 77.694 \n",
      "train epoch 2 loss : 90.611 \n",
      "train epoch 2 loss : 116.514 \n",
      "train epoch 2 loss : 74.347 \n",
      "train epoch 2 loss : 80.958 \n",
      "train epoch 2 loss : 71.165 \n",
      "train epoch 2 loss : 66.628 \n",
      "train epoch 2 loss : 106.555 \n",
      "train epoch 2 loss : 107.583 \n",
      "train epoch 2 loss : 103.595 \n",
      "train epoch 2 loss : 96.375 \n",
      "train epoch 2 loss : 129.537 \n",
      "train epoch 2 loss : 96.210 \n",
      "train epoch 2 loss : 65.471 \n",
      "train epoch 2 loss : 82.307 \n",
      "train epoch 2 loss : 98.070 \n",
      "train epoch 2 loss : 110.671 \n",
      "train epoch 2 loss : 60.405 \n",
      "train epoch 2 loss : 98.631 \n",
      "train epoch 2 loss : 97.237 \n",
      "train epoch 2 loss : 84.486 \n",
      "train epoch 2 loss : 97.336 \n",
      "train epoch 2 loss : 64.150 \n",
      "train epoch 2 loss : 87.191 \n",
      "train epoch 2 loss : 54.213 \n",
      "train epoch 2 loss : 54.721 \n",
      "train epoch 2 loss : 68.484 \n",
      "train epoch 2 loss : 84.784 \n",
      "train epoch 2 loss : 106.623 \n",
      "train epoch 2 loss : 84.024 \n",
      "train epoch 2 loss : 70.008 \n",
      "train epoch 2 loss : 72.801 \n",
      "train epoch 2 loss : 58.256 \n",
      "train epoch 2 loss : 78.142 \n",
      "train epoch 2 loss : 67.566 \n",
      "train epoch 2 loss : 67.849 \n",
      "train epoch 2 loss : 58.446 \n",
      "train epoch 2 loss : 57.122 \n",
      "train epoch 2 loss : 71.042 \n",
      "train epoch 2 loss : 74.565 \n",
      "train epoch 2 loss : 89.175 \n",
      "train epoch 2 loss : 71.234 \n",
      "train epoch 2 loss : 128.970 \n",
      "train epoch 2 loss : 134.010 \n",
      "train epoch 2 loss : 99.342 \n",
      "train epoch 2 loss : 144.289 \n",
      "train epoch 2 loss : 107.102 \n",
      "train epoch 2 loss : 78.149 \n",
      "train epoch 2 loss : 88.852 \n",
      "train epoch 2 loss : 71.951 \n",
      "train epoch 2 loss : 90.358 \n",
      "train epoch 2 loss : 93.764 \n",
      "train epoch 2 loss : 90.749 \n",
      "train epoch 2 loss : 126.697 \n",
      "train epoch 2 loss : 91.291 \n",
      "train epoch 2 loss : 57.033 \n",
      "train epoch 2 loss : 59.649 \n",
      "train epoch 2 loss : 65.021 \n",
      "train epoch 2 loss : 75.328 \n",
      "train epoch 2 loss : 55.457 \n",
      "train epoch 2 loss : 78.020 \n",
      "train epoch 2 loss : 57.516 \n",
      "train epoch 2 loss : 78.085 \n",
      "train epoch 2 loss : 72.818 \n",
      "train epoch 2 loss : 47.104 \n",
      "train epoch 2 loss : 41.906 \n",
      "train epoch 2 loss : 63.578 \n",
      "train epoch 2 loss : 77.940 \n",
      "train epoch 2 loss : 78.676 \n",
      "train epoch 2 loss : 73.723 \n",
      "train epoch 2 loss : 61.817 \n",
      "train epoch 2 loss : 52.412 \n",
      "train epoch 2 loss : 55.734 \n",
      "train epoch 2 loss : 74.132 \n",
      "train epoch 2 loss : 61.275 \n",
      "train epoch 2 loss : 57.459 \n",
      "train epoch 2 loss : 46.794 \n",
      "train epoch 2 loss : 51.735 \n",
      "train epoch 2 loss : 53.984 \n",
      "train epoch 2 loss : 92.901 \n",
      "train epoch 2 loss : 54.494 \n",
      "train epoch 2 loss : 66.519 \n",
      "train epoch 2 loss : 64.101 \n",
      "train epoch 2 loss : 63.273 \n",
      "train epoch 2 loss : 65.602 \n",
      "train epoch 2 loss : 64.216 \n",
      "train epoch 2 loss : 79.539 \n",
      "train epoch 2 loss : 62.267 \n",
      "train epoch 2 loss : 75.879 \n",
      "train epoch 2 loss : 50.038 \n",
      "train epoch 2 loss : 83.656 \n",
      "train epoch 2 loss : 94.544 \n",
      "train epoch 2 loss : 66.567 \n",
      "train epoch 2 loss : 52.478 \n",
      "train epoch 2 loss : 63.243 \n",
      "train epoch 2 loss : 66.526 \n",
      "train epoch 2 loss : 79.566 \n",
      "train epoch 2 loss : 81.072 \n",
      "train epoch 2 loss : 88.632 \n",
      "train epoch 2 loss : 99.841 \n",
      "train epoch 2 loss : 52.896 \n",
      "train epoch 2 loss : 73.183 \n",
      "train epoch 2 loss : 44.112 \n",
      "train epoch 2 loss : 60.162 \n",
      "train epoch 3 loss : 54.685 \n",
      "train epoch 3 loss : 100.186 \n",
      "train epoch 3 loss : 88.121 \n",
      "train epoch 3 loss : 113.872 \n",
      "train epoch 3 loss : 113.698 \n",
      "train epoch 3 loss : 66.569 \n",
      "train epoch 3 loss : 75.135 \n",
      "train epoch 3 loss : 72.348 \n",
      "train epoch 3 loss : 74.927 \n",
      "train epoch 3 loss : 55.134 \n",
      "train epoch 3 loss : 67.878 \n",
      "train epoch 3 loss : 50.546 \n",
      "train epoch 3 loss : 82.136 \n",
      "train epoch 3 loss : 44.858 \n",
      "train epoch 3 loss : 81.768 \n",
      "train epoch 3 loss : 69.622 \n",
      "train epoch 3 loss : 45.989 \n",
      "train epoch 3 loss : 60.966 \n",
      "train epoch 3 loss : 63.308 \n",
      "train epoch 3 loss : 65.784 \n",
      "train epoch 3 loss : 60.970 \n",
      "train epoch 3 loss : 63.080 \n",
      "train epoch 3 loss : 78.878 \n",
      "train epoch 3 loss : 97.712 \n",
      "train epoch 3 loss : 54.266 \n",
      "train epoch 3 loss : 68.859 \n",
      "train epoch 3 loss : 74.767 \n",
      "train epoch 3 loss : 89.378 \n",
      "train epoch 3 loss : 86.185 \n",
      "train epoch 3 loss : 105.652 \n",
      "train epoch 3 loss : 70.017 \n",
      "train epoch 3 loss : 66.953 \n",
      "train epoch 3 loss : 77.870 \n",
      "train epoch 3 loss : 125.785 \n",
      "train epoch 3 loss : 70.095 \n",
      "train epoch 3 loss : 72.244 \n",
      "train epoch 3 loss : 126.328 \n",
      "train epoch 3 loss : 70.684 \n",
      "train epoch 3 loss : 114.030 \n",
      "train epoch 3 loss : 85.589 \n",
      "train epoch 3 loss : 97.370 \n",
      "train epoch 3 loss : 80.571 \n",
      "train epoch 3 loss : 44.901 \n",
      "train epoch 3 loss : 83.629 \n",
      "train epoch 3 loss : 65.261 \n",
      "train epoch 3 loss : 79.267 \n",
      "train epoch 3 loss : 62.619 \n",
      "train epoch 3 loss : 78.603 \n",
      "train epoch 3 loss : 49.747 \n",
      "train epoch 3 loss : 63.018 \n",
      "train epoch 3 loss : 67.652 \n",
      "train epoch 3 loss : 81.541 \n",
      "train epoch 3 loss : 80.489 \n",
      "train epoch 3 loss : 82.044 \n",
      "train epoch 3 loss : 69.061 \n",
      "train epoch 3 loss : 66.817 \n",
      "train epoch 3 loss : 45.073 \n",
      "train epoch 3 loss : 66.863 \n",
      "train epoch 3 loss : 55.823 \n",
      "train epoch 3 loss : 59.144 \n",
      "train epoch 3 loss : 47.642 \n",
      "train epoch 3 loss : 44.941 \n",
      "train epoch 3 loss : 74.877 \n",
      "train epoch 3 loss : 52.010 \n",
      "train epoch 3 loss : 71.561 \n",
      "train epoch 3 loss : 41.126 \n",
      "train epoch 3 loss : 61.553 \n",
      "train epoch 3 loss : 44.946 \n",
      "train epoch 3 loss : 58.532 \n",
      "train epoch 3 loss : 63.618 \n",
      "train epoch 3 loss : 52.033 \n",
      "train epoch 3 loss : 53.572 \n",
      "train epoch 3 loss : 96.305 \n",
      "train epoch 3 loss : 66.732 \n",
      "train epoch 3 loss : 61.095 \n",
      "train epoch 3 loss : 69.453 \n",
      "train epoch 3 loss : 47.375 \n",
      "train epoch 3 loss : 91.523 \n",
      "train epoch 3 loss : 107.183 \n",
      "train epoch 3 loss : 56.602 \n",
      "train epoch 3 loss : 54.675 \n",
      "train epoch 3 loss : 77.766 \n",
      "train epoch 3 loss : 86.542 \n",
      "train epoch 3 loss : 44.689 \n",
      "train epoch 3 loss : 52.680 \n",
      "train epoch 3 loss : 69.627 \n",
      "train epoch 3 loss : 61.858 \n",
      "train epoch 3 loss : 52.355 \n",
      "train epoch 3 loss : 57.433 \n",
      "train epoch 3 loss : 76.895 \n",
      "train epoch 3 loss : 65.635 \n",
      "train epoch 3 loss : 72.941 \n",
      "train epoch 3 loss : 71.243 \n",
      "train epoch 3 loss : 72.131 \n",
      "train epoch 3 loss : 57.559 \n",
      "train epoch 3 loss : 55.214 \n",
      "train epoch 3 loss : 35.953 \n",
      "train epoch 3 loss : 55.964 \n",
      "train epoch 3 loss : 65.535 \n",
      "train epoch 3 loss : 65.161 \n",
      "train epoch 3 loss : 46.501 \n",
      "train epoch 3 loss : 58.980 \n",
      "train epoch 3 loss : 80.848 \n",
      "train epoch 3 loss : 45.113 \n",
      "train epoch 3 loss : 48.016 \n",
      "train epoch 3 loss : 68.820 \n",
      "train epoch 3 loss : 76.155 \n",
      "train epoch 3 loss : 64.411 \n",
      "train epoch 3 loss : 71.341 \n",
      "train epoch 3 loss : 46.737 \n",
      "train epoch 3 loss : 88.083 \n",
      "train epoch 3 loss : 70.747 \n",
      "train epoch 3 loss : 70.195 \n",
      "train epoch 3 loss : 49.990 \n",
      "train epoch 3 loss : 78.413 \n",
      "train epoch 3 loss : 48.403 \n",
      "train epoch 3 loss : 67.569 \n",
      "train epoch 3 loss : 72.249 \n",
      "train epoch 3 loss : 57.956 \n",
      "train epoch 3 loss : 53.466 \n",
      "train epoch 3 loss : 51.267 \n",
      "train epoch 3 loss : 33.354 \n",
      "train epoch 3 loss : 57.746 \n",
      "train epoch 3 loss : 43.584 \n",
      "train epoch 3 loss : 46.406 \n",
      "train epoch 3 loss : 48.496 \n",
      "train epoch 3 loss : 37.799 \n",
      "train epoch 3 loss : 52.181 \n",
      "train epoch 3 loss : 57.500 \n",
      "train epoch 3 loss : 54.833 \n",
      "train epoch 3 loss : 74.498 \n",
      "train epoch 3 loss : 63.171 \n",
      "train epoch 3 loss : 58.958 \n",
      "train epoch 3 loss : 36.835 \n",
      "train epoch 3 loss : 61.566 \n",
      "train epoch 3 loss : 51.613 \n",
      "train epoch 3 loss : 79.391 \n",
      "train epoch 3 loss : 51.170 \n",
      "train epoch 3 loss : 52.542 \n",
      "train epoch 3 loss : 54.756 \n",
      "train epoch 3 loss : 68.102 \n",
      "train epoch 3 loss : 70.862 \n",
      "train epoch 3 loss : 43.678 \n",
      "train epoch 3 loss : 50.896 \n",
      "train epoch 3 loss : 52.076 \n",
      "train epoch 3 loss : 43.008 \n",
      "train epoch 3 loss : 41.315 \n",
      "train epoch 3 loss : 43.514 \n",
      "train epoch 3 loss : 51.738 \n",
      "train epoch 3 loss : 55.355 \n",
      "train epoch 3 loss : 49.278 \n",
      "train epoch 3 loss : 52.011 \n",
      "train epoch 3 loss : 62.571 \n",
      "train epoch 3 loss : 61.364 \n",
      "train epoch 3 loss : 56.267 \n",
      "train epoch 3 loss : 44.055 \n",
      "train epoch 3 loss : 43.583 \n",
      "train epoch 3 loss : 43.283 \n",
      "train epoch 3 loss : 46.896 \n",
      "train epoch 3 loss : 47.818 \n",
      "train epoch 3 loss : 49.073 \n",
      "train epoch 3 loss : 38.407 \n",
      "train epoch 3 loss : 55.567 \n",
      "train epoch 3 loss : 55.146 \n",
      "train epoch 3 loss : 52.943 \n",
      "train epoch 3 loss : 47.917 \n",
      "train epoch 3 loss : 67.968 \n",
      "train epoch 3 loss : 34.100 \n",
      "train epoch 3 loss : 36.231 \n",
      "train epoch 3 loss : 35.396 \n",
      "train epoch 3 loss : 51.027 \n",
      "train epoch 3 loss : 32.129 \n",
      "train epoch 3 loss : 39.194 \n",
      "train epoch 3 loss : 80.964 \n",
      "train epoch 3 loss : 78.210 \n",
      "train epoch 3 loss : 52.208 \n",
      "train epoch 3 loss : 34.329 \n",
      "train epoch 3 loss : 33.815 \n",
      "train epoch 3 loss : 41.552 \n",
      "train epoch 3 loss : 45.143 \n",
      "train epoch 3 loss : 55.324 \n",
      "train epoch 3 loss : 60.102 \n",
      "train epoch 3 loss : 66.590 \n",
      "train epoch 3 loss : 57.734 \n",
      "train epoch 3 loss : 65.340 \n",
      "train epoch 3 loss : 58.881 \n",
      "train epoch 3 loss : 102.575 \n",
      "train epoch 3 loss : 76.260 \n",
      "train epoch 3 loss : 47.598 \n",
      "train epoch 3 loss : 83.451 \n",
      "train epoch 3 loss : 84.356 \n",
      "train epoch 3 loss : 70.599 \n",
      "train epoch 3 loss : 64.187 \n",
      "train epoch 3 loss : 39.271 \n",
      "train epoch 3 loss : 50.855 \n",
      "train epoch 3 loss : 63.796 \n",
      "train epoch 3 loss : 26.465 \n",
      "train epoch 3 loss : 72.881 \n",
      "train epoch 3 loss : 110.046 \n",
      "train epoch 3 loss : 68.490 \n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr =0.001)\n",
    "criteon= nn.MSELoss()\n",
    "EPOCH = 4\n",
    "loss_list = []\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()\n",
    "    sum_loss = 0 \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        #data, target = Variable(data), Variable(target)\n",
    "\n",
    "        data = torch.reshape(data,(batchsize, 1, 32,32))\n",
    "        target = torch.reshape(target, (batchsize, 1))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        data=data.to(torch.float32)\n",
    "        target=target.to(torch.float32)\n",
    "        output = model(data)\n",
    "\n",
    "        \n",
    "        #print(data)\n",
    "        #print(target)\n",
    "        #print(\"The output is:\")\n",
    "        #print(output)\n",
    "        \n",
    "        loss = criteon(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "        if batch_idx % 10 == 9:\n",
    "            print(\"train epoch {} loss : {:.3f} \".format(epoch, sum_loss/10))\n",
    "            loss_list.append(sum_loss/100)\n",
    "            sum_loss = 0      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 55.43486  ,   5.557933 ,  49.076565 , ...,   5.5215306,\n",
       "       -36.24969  ,  -3.6554797], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.cpu()\n",
    "re = model(torch.Tensor(X_test.reshape(4000,1,32,32))).detach().numpy()\n",
    "re = re.reshape(4000)\n",
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 61.5076,   4.2478,  48.3385,  ...,   3.4108, -27.6324,  -7.8302])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.434860</td>\n",
       "      <td>61.507622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.557933</td>\n",
       "      <td>4.247818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49.076565</td>\n",
       "      <td>48.338482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.994368</td>\n",
       "      <td>23.041401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.268932</td>\n",
       "      <td>68.316994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>5.268528</td>\n",
       "      <td>-6.869918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>6.880189</td>\n",
       "      <td>4.799513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>5.521531</td>\n",
       "      <td>3.410830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>-36.249691</td>\n",
       "      <td>-27.632389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>-3.655480</td>\n",
       "      <td>-7.830187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pre       true\n",
       "0     55.434860  61.507622\n",
       "1      5.557933   4.247818\n",
       "2     49.076565  48.338482\n",
       "3     25.994368  23.041401\n",
       "4     55.268932  68.316994\n",
       "...         ...        ...\n",
       "3995   5.268528  -6.869918\n",
       "3996   6.880189   4.799513\n",
       "3997   5.521531   3.410830\n",
       "3998 -36.249691 -27.632389\n",
       "3999  -3.655480  -7.830187\n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultvalue = pd.DataFrame([re,Y_test.numpy()]).T\n",
    "resultvalue.columns = ['pre','true']\n",
    "resultvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre</th>\n",
       "      <th>true</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.434860</td>\n",
       "      <td>61.507622</td>\n",
       "      <td>0.098732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.557933</td>\n",
       "      <td>4.247818</td>\n",
       "      <td>0.308421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49.076565</td>\n",
       "      <td>48.338482</td>\n",
       "      <td>0.015269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.994368</td>\n",
       "      <td>23.041401</td>\n",
       "      <td>0.128159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.268932</td>\n",
       "      <td>68.316994</td>\n",
       "      <td>0.190993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>5.268528</td>\n",
       "      <td>-6.869918</td>\n",
       "      <td>1.766898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>6.880189</td>\n",
       "      <td>4.799513</td>\n",
       "      <td>0.433518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>5.521531</td>\n",
       "      <td>3.410830</td>\n",
       "      <td>0.618823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>-36.249691</td>\n",
       "      <td>-27.632389</td>\n",
       "      <td>0.311855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>-3.655480</td>\n",
       "      <td>-7.830187</td>\n",
       "      <td>0.533155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pre       true      mape\n",
       "0     55.434860  61.507622  0.098732\n",
       "1      5.557933   4.247818  0.308421\n",
       "2     49.076565  48.338482  0.015269\n",
       "3     25.994368  23.041401  0.128159\n",
       "4     55.268932  68.316994  0.190993\n",
       "...         ...        ...       ...\n",
       "3995   5.268528  -6.869918  1.766898\n",
       "3996   6.880189   4.799513  0.433518\n",
       "3997   5.521531   3.410830  0.618823\n",
       "3998 -36.249691 -27.632389  0.311855\n",
       "3999  -3.655480  -7.830187  0.533155\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultvalue['mape'] = np.abs(resultvalue['pre'] - resultvalue['true'])/np.abs(resultvalue['true'])\n",
    "resultvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17648991546688447"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultvalue['mape'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultvalue.to_csv(\"hhh.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '2d32-80-20model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
