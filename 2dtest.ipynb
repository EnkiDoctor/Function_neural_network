{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch import nn,optim\n",
    "from torch.nn import functional as F \n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = np.load('2ddata.npy')\n",
    "trainlabel = np.load('2dlabel.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 43.52606322, -13.99713994,   9.49569717, ...,  -7.8036471 ,\n",
       "       -25.3031973 ,  -5.26887027])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(traindata, trainlabel, test_size = 0.2, random_state = 1)\n",
    "X_train, Y_train = torch.FloatTensor(X_train), torch.FloatTensor(Y_train)\n",
    "X_test, Y_test = torch.FloatTensor(X_test), torch.FloatTensor(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 16\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "\n",
    "#dataloader \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batchsize, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batchsize, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8000, 256, 256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_16d2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_16d2,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, 2,  stride=2)\n",
    "        self.conv2 = nn.Conv2d(4, 16, 2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(16, 64, 2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(64, 256, 2, stride=2)\n",
    "        self.conv5 = nn.Conv2d(256, 640, 2, stride=2)\n",
    "        self.conv6 = nn.Conv2d(640, 160, 2, stride=2)\n",
    "        self.conv7 = nn.Conv2d(160, 40, 2, stride=2)\n",
    "        self.conv8 = nn.Conv2d(40, 10, 2, stride=2)\n",
    "\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(10, 120)\n",
    "        self.fc2 = nn.Linear(120, 60)\n",
    "        self.fc3 = nn.Linear(60, 20)\n",
    "        self.fc4 = nn.Linear(20, 1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv4(x)  \n",
    "        #print(x.shape)\n",
    "        x = self.conv5(x)  \n",
    "        #print(x.shape)\n",
    "        x = self.conv6(x)  \n",
    "        #print(x.shape)\n",
    "        x = self.conv7(x)  \n",
    "        #print(x.shape)\n",
    "        x = self.conv8(x)  \n",
    "        #print(x.shape)\n",
    "        x = self.flat(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_available\n"
     ]
    }
   ],
   "source": [
    "model = CNN_16d2()\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu_available\")\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 0 loss : 783028705480066203648.000 ^e6\n",
      "train epoch 0 loss : 803770231157373140992.000 ^e6\n",
      "train epoch 0 loss : 774009698648911118336.000 ^e6\n",
      "train epoch 0 loss : 786005982303446761472.000 ^e6\n",
      "train epoch 0 loss : 784792664683003117568.000 ^e6\n",
      "train epoch 1 loss : 789721949769887121408.000 ^e6\n",
      "train epoch 1 loss : 788722601836912443392.000 ^e6\n",
      "train epoch 1 loss : 791562288454531022848.000 ^e6\n",
      "train epoch 1 loss : 778458183371039309824.000 ^e6\n",
      "train epoch 1 loss : 788637168324279271424.000 ^e6\n",
      "train epoch 2 loss : 780100709502413438976.000 ^e6\n",
      "train epoch 2 loss : 790184612557999833088.000 ^e6\n",
      "train epoch 2 loss : 790214091930769162240.000 ^e6\n",
      "train epoch 2 loss : 794678945555718602752.000 ^e6\n",
      "train epoch 2 loss : 779456620034856714240.000 ^e6\n",
      "train epoch 3 loss : 774286773172116783104.000 ^e6\n",
      "train epoch 3 loss : 796129810016166871040.000 ^e6\n",
      "train epoch 3 loss : 798250554337061240832.000 ^e6\n",
      "train epoch 3 loss : 782579238016924254208.000 ^e6\n",
      "train epoch 3 loss : 781033235211785732096.000 ^e6\n",
      "train epoch 4 loss : 791014901246627282944.000 ^e6\n",
      "train epoch 4 loss : 780598915108072456192.000 ^e6\n",
      "train epoch 4 loss : 782383822433579433984.000 ^e6\n",
      "train epoch 4 loss : 785652453404753264640.000 ^e6\n",
      "train epoch 4 loss : 790320464711535296512.000 ^e6\n",
      "train epoch 5 loss : 794848553775169404928.000 ^e6\n",
      "train epoch 5 loss : 784789440561073881088.000 ^e6\n",
      "train epoch 5 loss : 784758185504720027648.000 ^e6\n",
      "train epoch 5 loss : 774781839510869377024.000 ^e6\n",
      "train epoch 5 loss : 788486704135828144128.000 ^e6\n",
      "train epoch 6 loss : 771005446413801750528.000 ^e6\n",
      "train epoch 6 loss : 788649881082425114624.000 ^e6\n",
      "train epoch 6 loss : 795504209351260372992.000 ^e6\n",
      "train epoch 6 loss : 788723186229764685824.000 ^e6\n",
      "train epoch 6 loss : 781502915879213727744.000 ^e6\n",
      "train epoch 7 loss : 782551250616815714304.000 ^e6\n",
      "train epoch 7 loss : 787131422217667477504.000 ^e6\n",
      "train epoch 7 loss : 792730736214689710080.000 ^e6\n",
      "train epoch 7 loss : 775396729258296868864.000 ^e6\n",
      "train epoch 7 loss : 785309230333038690304.000 ^e6\n",
      "train epoch 8 loss : 778340603086443773952.000 ^e6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\github_project\\Function_neural_network\\2dtest.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/Function_neural_network/2dtest.ipynb#ch0000008?line=17'>18</a>\u001b[0m data\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/Function_neural_network/2dtest.ipynb#ch0000008?line=18'>19</a>\u001b[0m target\u001b[39m=\u001b[39mtarget\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/github_project/Function_neural_network/2dtest.ipynb#ch0000008?line=19'>20</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/Function_neural_network/2dtest.ipynb#ch0000008?line=22'>23</a>\u001b[0m \u001b[39m#print(data)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/Function_neural_network/2dtest.ipynb#ch0000008?line=23'>24</a>\u001b[0m \u001b[39m#print(target)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/Function_neural_network/2dtest.ipynb#ch0000008?line=24'>25</a>\u001b[0m \u001b[39m#print(\"The output is:\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/Function_neural_network/2dtest.ipynb#ch0000008?line=25'>26</a>\u001b[0m \u001b[39m#print(output)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/Function_neural_network/2dtest.ipynb#ch0000008?line=27'>28</a>\u001b[0m loss \u001b[39m=\u001b[39m criteon(output, target)\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32me:\\github_project\\Function_neural_network\\2dtest.ipynb Cell 7'\u001b[0m in \u001b[0;36mCNN_16d2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/Function_neural_network/2dtest.ipynb#ch0000006?line=37'>38</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/Function_neural_network/2dtest.ipynb#ch0000006?line=38'>39</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/github_project/Function_neural_network/2dtest.ipynb#ch0000006?line=39'>40</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc3(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/Function_neural_network/2dtest.ipynb#ch0000006?line=40'>41</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc4(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github_project/Function_neural_network/2dtest.ipynb#ch0000006?line=41'>42</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/functional.py?line=1845'>1846</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/functional.py?line=1846'>1847</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda3/lib/site-packages/torch/nn/functional.py?line=1847'>1848</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr =1)\n",
    "criteon= nn.MSELoss()\n",
    "EPOCH = 100\n",
    "loss_list = []\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()\n",
    "    sum_loss = 0 \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        #data, target = Variable(data), Variable(target)\n",
    "\n",
    "        data = torch.reshape(data,(batchsize, 1, 256,256))\n",
    "        target = torch.reshape(target, (batchsize, 1))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        data=data.to(torch.float32)\n",
    "        target=target.to(torch.float32)\n",
    "        output = model(data)\n",
    "\n",
    "        \n",
    "        #print(data)\n",
    "        #print(target)\n",
    "        #print(\"The output is:\")\n",
    "        #print(output)\n",
    "        \n",
    "        loss = criteon(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "        if batch_idx % 100 == 99:\n",
    "            print(\"train epoch {} loss : {:.3f} ^e6\".format(epoch, sum_loss/100000000))\n",
    "            loss_list.append(sum_loss/100)\n",
    "            sum_loss = 0      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\github_project\\Function_neural_network\\2dtest.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/github_project/Function_neural_network/2dtest.ipynb#ch0000009?line=0'>1</a>\u001b[0m label\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.13073777e+05, -1.19680497e+04, -3.54062587e+03, ...,\n",
       "        4.05250728e+02, -9.34886412e+03,  2.26318108e+04])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
