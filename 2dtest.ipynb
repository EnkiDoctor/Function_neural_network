{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch import nn,optim\n",
    "from torch.nn import functional as F \n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = np.load('2ddata.npy')\n",
    "trainlabel = np.load('2dlabel.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 43.52606322, -13.99713994,   9.49569717, ..., -12.22158433,\n",
       "         6.01721102, -24.02406788])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(traindata, trainlabel, test_size = 0.2, random_state = 1)\n",
    "X_train, Y_train = torch.FloatTensor(X_train), torch.FloatTensor(Y_train)\n",
    "X_test, Y_test = torch.FloatTensor(X_test), torch.FloatTensor(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 8\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "\n",
    "#dataloader \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batchsize, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batchsize, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16000, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_16d2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_16d2,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, 2,  stride=2)\n",
    "        self.conv2 = nn.Conv2d(4, 16, 2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(16, 160, 2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(160, 40, 2, stride=2)\n",
    "        self.conv5 = nn.Conv2d(40, 10, 2, stride=2)\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(10, 60)\n",
    "        self.fc2 = nn.Linear(60, 60)\n",
    "        self.fc3 = nn.Linear(60, 20)\n",
    "        self.fc4 = nn.Linear(20, 1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv4(x)  \n",
    "        #print(x.shape)\n",
    "        x = self.conv5(x)  \n",
    "        #print(x.shape)\n",
    "\n",
    "        x = self.flat(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2673]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(1,1,32,32)\n",
    "xiao = CNN_16d2()\n",
    "xiao(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_available\n"
     ]
    }
   ],
   "source": [
    "model = CNN_16d2()\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu_available\")\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 0 loss : 1754.225 \n",
      "train epoch 0 loss : 1439.253 \n",
      "train epoch 0 loss : 1851.075 \n",
      "train epoch 0 loss : 1535.793 \n",
      "train epoch 0 loss : 1619.769 \n",
      "train epoch 0 loss : 1710.460 \n",
      "train epoch 0 loss : 2180.364 \n",
      "train epoch 0 loss : 1733.705 \n",
      "train epoch 0 loss : 842.527 \n",
      "train epoch 0 loss : 2245.000 \n",
      "train epoch 0 loss : 1624.936 \n",
      "train epoch 0 loss : 1783.496 \n",
      "train epoch 0 loss : 1345.624 \n",
      "train epoch 0 loss : 1216.751 \n",
      "train epoch 0 loss : 1257.455 \n",
      "train epoch 0 loss : 1345.824 \n",
      "train epoch 0 loss : 1869.114 \n",
      "train epoch 0 loss : 1772.347 \n",
      "train epoch 0 loss : 1557.747 \n",
      "train epoch 0 loss : 2129.412 \n",
      "train epoch 0 loss : 1693.431 \n",
      "train epoch 0 loss : 1510.720 \n",
      "train epoch 0 loss : 1661.569 \n",
      "train epoch 0 loss : 1704.308 \n",
      "train epoch 0 loss : 1928.299 \n",
      "train epoch 0 loss : 1367.831 \n",
      "train epoch 0 loss : 1706.397 \n",
      "train epoch 0 loss : 1347.812 \n",
      "train epoch 0 loss : 1137.558 \n",
      "train epoch 0 loss : 1822.982 \n",
      "train epoch 0 loss : 1901.612 \n",
      "train epoch 0 loss : 961.448 \n",
      "train epoch 0 loss : 1613.191 \n",
      "train epoch 0 loss : 1198.338 \n",
      "train epoch 0 loss : 1233.501 \n",
      "train epoch 0 loss : 1025.090 \n",
      "train epoch 0 loss : 1363.671 \n",
      "train epoch 0 loss : 1403.006 \n",
      "train epoch 0 loss : 1395.708 \n",
      "train epoch 0 loss : 1881.607 \n",
      "train epoch 0 loss : 1438.425 \n",
      "train epoch 0 loss : 1142.341 \n",
      "train epoch 0 loss : 1428.563 \n",
      "train epoch 0 loss : 1695.090 \n",
      "train epoch 0 loss : 1290.636 \n",
      "train epoch 0 loss : 1505.107 \n",
      "train epoch 0 loss : 1498.591 \n",
      "train epoch 0 loss : 1307.391 \n",
      "train epoch 0 loss : 1671.834 \n",
      "train epoch 0 loss : 995.657 \n",
      "train epoch 0 loss : 1416.954 \n",
      "train epoch 0 loss : 1227.557 \n",
      "train epoch 0 loss : 1404.768 \n",
      "train epoch 0 loss : 1086.616 \n",
      "train epoch 0 loss : 1424.742 \n",
      "train epoch 0 loss : 1142.344 \n",
      "train epoch 0 loss : 1147.552 \n",
      "train epoch 0 loss : 1373.501 \n",
      "train epoch 0 loss : 795.372 \n",
      "train epoch 0 loss : 665.508 \n",
      "train epoch 0 loss : 429.517 \n",
      "train epoch 0 loss : 461.507 \n",
      "train epoch 0 loss : 448.329 \n",
      "train epoch 0 loss : 402.986 \n",
      "train epoch 0 loss : 417.493 \n",
      "train epoch 0 loss : 447.464 \n",
      "train epoch 0 loss : 406.030 \n",
      "train epoch 0 loss : 580.698 \n",
      "train epoch 0 loss : 558.465 \n",
      "train epoch 0 loss : 671.186 \n",
      "train epoch 0 loss : 317.819 \n",
      "train epoch 0 loss : 266.597 \n",
      "train epoch 0 loss : 321.410 \n",
      "train epoch 0 loss : 237.351 \n",
      "train epoch 0 loss : 303.167 \n",
      "train epoch 0 loss : 453.944 \n",
      "train epoch 0 loss : 363.612 \n",
      "train epoch 0 loss : 269.109 \n",
      "train epoch 0 loss : 276.077 \n",
      "train epoch 0 loss : 279.529 \n",
      "train epoch 0 loss : 331.634 \n",
      "train epoch 0 loss : 283.342 \n",
      "train epoch 0 loss : 243.264 \n",
      "train epoch 0 loss : 253.139 \n",
      "train epoch 0 loss : 251.463 \n",
      "train epoch 0 loss : 282.310 \n",
      "train epoch 0 loss : 226.524 \n",
      "train epoch 0 loss : 357.761 \n",
      "train epoch 0 loss : 178.610 \n",
      "train epoch 0 loss : 287.732 \n",
      "train epoch 0 loss : 195.581 \n",
      "train epoch 0 loss : 170.133 \n",
      "train epoch 0 loss : 141.684 \n",
      "train epoch 0 loss : 140.736 \n",
      "train epoch 0 loss : 259.930 \n",
      "train epoch 0 loss : 182.308 \n",
      "train epoch 0 loss : 159.413 \n",
      "train epoch 0 loss : 101.257 \n",
      "train epoch 0 loss : 160.591 \n",
      "train epoch 0 loss : 109.000 \n",
      "train epoch 0 loss : 97.530 \n",
      "train epoch 0 loss : 152.193 \n",
      "train epoch 0 loss : 124.340 \n",
      "train epoch 0 loss : 164.411 \n",
      "train epoch 0 loss : 175.338 \n",
      "train epoch 0 loss : 158.410 \n",
      "train epoch 0 loss : 150.028 \n",
      "train epoch 0 loss : 180.259 \n",
      "train epoch 0 loss : 126.987 \n",
      "train epoch 0 loss : 188.616 \n",
      "train epoch 0 loss : 140.481 \n",
      "train epoch 0 loss : 124.690 \n",
      "train epoch 0 loss : 172.150 \n",
      "train epoch 0 loss : 158.739 \n",
      "train epoch 0 loss : 91.794 \n",
      "train epoch 0 loss : 109.252 \n",
      "train epoch 0 loss : 76.796 \n",
      "train epoch 0 loss : 128.540 \n",
      "train epoch 0 loss : 127.057 \n",
      "train epoch 0 loss : 127.939 \n",
      "train epoch 0 loss : 136.552 \n",
      "train epoch 0 loss : 105.078 \n",
      "train epoch 0 loss : 125.361 \n",
      "train epoch 0 loss : 110.547 \n",
      "train epoch 0 loss : 148.984 \n",
      "train epoch 0 loss : 139.954 \n",
      "train epoch 0 loss : 102.765 \n",
      "train epoch 0 loss : 97.469 \n",
      "train epoch 0 loss : 109.042 \n",
      "train epoch 0 loss : 112.921 \n",
      "train epoch 0 loss : 163.432 \n",
      "train epoch 0 loss : 160.381 \n",
      "train epoch 0 loss : 166.286 \n",
      "train epoch 0 loss : 107.041 \n",
      "train epoch 0 loss : 169.994 \n",
      "train epoch 0 loss : 109.865 \n",
      "train epoch 0 loss : 107.624 \n",
      "train epoch 0 loss : 105.782 \n",
      "train epoch 0 loss : 113.153 \n",
      "train epoch 0 loss : 130.354 \n",
      "train epoch 0 loss : 77.234 \n",
      "train epoch 0 loss : 107.417 \n",
      "train epoch 0 loss : 126.429 \n",
      "train epoch 0 loss : 86.585 \n",
      "train epoch 0 loss : 113.992 \n",
      "train epoch 0 loss : 123.348 \n",
      "train epoch 0 loss : 116.314 \n",
      "train epoch 0 loss : 102.493 \n",
      "train epoch 0 loss : 138.589 \n",
      "train epoch 0 loss : 112.083 \n",
      "train epoch 0 loss : 152.975 \n",
      "train epoch 0 loss : 140.166 \n",
      "train epoch 0 loss : 88.198 \n",
      "train epoch 0 loss : 105.386 \n",
      "train epoch 0 loss : 128.342 \n",
      "train epoch 0 loss : 143.995 \n",
      "train epoch 0 loss : 126.184 \n",
      "train epoch 0 loss : 76.285 \n",
      "train epoch 0 loss : 79.534 \n",
      "train epoch 0 loss : 96.831 \n",
      "train epoch 0 loss : 140.801 \n",
      "train epoch 0 loss : 88.348 \n",
      "train epoch 0 loss : 103.162 \n",
      "train epoch 0 loss : 104.521 \n",
      "train epoch 0 loss : 92.340 \n",
      "train epoch 0 loss : 74.725 \n",
      "train epoch 0 loss : 93.855 \n",
      "train epoch 0 loss : 97.448 \n",
      "train epoch 0 loss : 100.973 \n",
      "train epoch 0 loss : 90.868 \n",
      "train epoch 0 loss : 125.293 \n",
      "train epoch 0 loss : 125.104 \n",
      "train epoch 0 loss : 91.219 \n",
      "train epoch 0 loss : 109.159 \n",
      "train epoch 0 loss : 161.152 \n",
      "train epoch 0 loss : 145.294 \n",
      "train epoch 0 loss : 188.352 \n",
      "train epoch 0 loss : 217.417 \n",
      "train epoch 0 loss : 106.821 \n",
      "train epoch 0 loss : 108.683 \n",
      "train epoch 0 loss : 114.072 \n",
      "train epoch 0 loss : 100.479 \n",
      "train epoch 0 loss : 96.546 \n",
      "train epoch 0 loss : 98.751 \n",
      "train epoch 0 loss : 116.915 \n",
      "train epoch 0 loss : 113.149 \n",
      "train epoch 0 loss : 101.356 \n",
      "train epoch 0 loss : 76.512 \n",
      "train epoch 0 loss : 88.851 \n",
      "train epoch 0 loss : 96.259 \n",
      "train epoch 0 loss : 93.852 \n",
      "train epoch 0 loss : 117.034 \n",
      "train epoch 0 loss : 108.158 \n",
      "train epoch 0 loss : 82.699 \n",
      "train epoch 0 loss : 97.519 \n",
      "train epoch 0 loss : 72.007 \n",
      "train epoch 0 loss : 93.623 \n",
      "train epoch 0 loss : 132.743 \n",
      "train epoch 0 loss : 94.169 \n",
      "train epoch 0 loss : 166.996 \n",
      "train epoch 1 loss : 147.171 \n",
      "train epoch 1 loss : 124.251 \n",
      "train epoch 1 loss : 73.845 \n",
      "train epoch 1 loss : 141.530 \n",
      "train epoch 1 loss : 90.157 \n",
      "train epoch 1 loss : 122.008 \n",
      "train epoch 1 loss : 184.681 \n",
      "train epoch 1 loss : 130.815 \n",
      "train epoch 1 loss : 132.544 \n",
      "train epoch 1 loss : 105.409 \n",
      "train epoch 1 loss : 74.148 \n",
      "train epoch 1 loss : 72.653 \n",
      "train epoch 1 loss : 75.674 \n",
      "train epoch 1 loss : 71.803 \n",
      "train epoch 1 loss : 55.458 \n",
      "train epoch 1 loss : 85.698 \n",
      "train epoch 1 loss : 106.131 \n",
      "train epoch 1 loss : 48.854 \n",
      "train epoch 1 loss : 76.532 \n",
      "train epoch 1 loss : 100.188 \n",
      "train epoch 1 loss : 73.041 \n",
      "train epoch 1 loss : 70.738 \n",
      "train epoch 1 loss : 98.070 \n",
      "train epoch 1 loss : 84.723 \n",
      "train epoch 1 loss : 66.395 \n",
      "train epoch 1 loss : 87.814 \n",
      "train epoch 1 loss : 62.807 \n",
      "train epoch 1 loss : 99.178 \n",
      "train epoch 1 loss : 59.256 \n",
      "train epoch 1 loss : 111.206 \n",
      "train epoch 1 loss : 78.509 \n",
      "train epoch 1 loss : 70.560 \n",
      "train epoch 1 loss : 42.026 \n",
      "train epoch 1 loss : 67.795 \n",
      "train epoch 1 loss : 69.915 \n",
      "train epoch 1 loss : 82.271 \n",
      "train epoch 1 loss : 105.663 \n",
      "train epoch 1 loss : 112.333 \n",
      "train epoch 1 loss : 70.738 \n",
      "train epoch 1 loss : 213.241 \n",
      "train epoch 1 loss : 100.156 \n",
      "train epoch 1 loss : 93.037 \n",
      "train epoch 1 loss : 95.280 \n",
      "train epoch 1 loss : 62.313 \n",
      "train epoch 1 loss : 51.428 \n",
      "train epoch 1 loss : 79.466 \n",
      "train epoch 1 loss : 72.466 \n",
      "train epoch 1 loss : 78.007 \n",
      "train epoch 1 loss : 105.180 \n",
      "train epoch 1 loss : 71.151 \n",
      "train epoch 1 loss : 103.220 \n",
      "train epoch 1 loss : 108.191 \n",
      "train epoch 1 loss : 115.131 \n",
      "train epoch 1 loss : 131.640 \n",
      "train epoch 1 loss : 77.804 \n",
      "train epoch 1 loss : 68.404 \n",
      "train epoch 1 loss : 64.138 \n",
      "train epoch 1 loss : 59.186 \n",
      "train epoch 1 loss : 87.057 \n",
      "train epoch 1 loss : 55.853 \n",
      "train epoch 1 loss : 54.734 \n",
      "train epoch 1 loss : 67.800 \n",
      "train epoch 1 loss : 85.104 \n",
      "train epoch 1 loss : 90.379 \n",
      "train epoch 1 loss : 68.741 \n",
      "train epoch 1 loss : 84.357 \n",
      "train epoch 1 loss : 90.350 \n",
      "train epoch 1 loss : 74.818 \n",
      "train epoch 1 loss : 92.643 \n",
      "train epoch 1 loss : 63.433 \n",
      "train epoch 1 loss : 67.300 \n",
      "train epoch 1 loss : 48.747 \n",
      "train epoch 1 loss : 107.258 \n",
      "train epoch 1 loss : 94.316 \n",
      "train epoch 1 loss : 82.941 \n",
      "train epoch 1 loss : 64.118 \n",
      "train epoch 1 loss : 80.779 \n",
      "train epoch 1 loss : 120.704 \n",
      "train epoch 1 loss : 98.658 \n",
      "train epoch 1 loss : 103.127 \n",
      "train epoch 1 loss : 95.141 \n",
      "train epoch 1 loss : 80.043 \n",
      "train epoch 1 loss : 79.977 \n",
      "train epoch 1 loss : 73.230 \n",
      "train epoch 1 loss : 72.758 \n",
      "train epoch 1 loss : 71.039 \n",
      "train epoch 1 loss : 98.020 \n",
      "train epoch 1 loss : 91.693 \n",
      "train epoch 1 loss : 57.432 \n",
      "train epoch 1 loss : 70.056 \n",
      "train epoch 1 loss : 65.945 \n",
      "train epoch 1 loss : 81.396 \n",
      "train epoch 1 loss : 77.474 \n",
      "train epoch 1 loss : 136.827 \n",
      "train epoch 1 loss : 153.820 \n",
      "train epoch 1 loss : 123.075 \n",
      "train epoch 1 loss : 95.212 \n",
      "train epoch 1 loss : 99.898 \n",
      "train epoch 1 loss : 102.701 \n",
      "train epoch 1 loss : 76.861 \n",
      "train epoch 1 loss : 66.363 \n",
      "train epoch 1 loss : 86.657 \n",
      "train epoch 1 loss : 67.533 \n",
      "train epoch 1 loss : 91.855 \n",
      "train epoch 1 loss : 87.832 \n",
      "train epoch 1 loss : 86.589 \n",
      "train epoch 1 loss : 70.938 \n",
      "train epoch 1 loss : 52.367 \n",
      "train epoch 1 loss : 66.663 \n",
      "train epoch 1 loss : 102.419 \n",
      "train epoch 1 loss : 85.353 \n",
      "train epoch 1 loss : 53.079 \n",
      "train epoch 1 loss : 57.222 \n",
      "train epoch 1 loss : 84.855 \n",
      "train epoch 1 loss : 85.421 \n",
      "train epoch 1 loss : 59.496 \n",
      "train epoch 1 loss : 90.246 \n",
      "train epoch 1 loss : 54.920 \n",
      "train epoch 1 loss : 102.465 \n",
      "train epoch 1 loss : 91.769 \n",
      "train epoch 1 loss : 90.248 \n",
      "train epoch 1 loss : 81.113 \n",
      "train epoch 1 loss : 56.821 \n",
      "train epoch 1 loss : 56.831 \n",
      "train epoch 1 loss : 63.156 \n",
      "train epoch 1 loss : 51.468 \n",
      "train epoch 1 loss : 100.046 \n",
      "train epoch 1 loss : 71.777 \n",
      "train epoch 1 loss : 65.862 \n",
      "train epoch 1 loss : 101.190 \n",
      "train epoch 1 loss : 67.211 \n",
      "train epoch 1 loss : 64.691 \n",
      "train epoch 1 loss : 118.713 \n",
      "train epoch 1 loss : 67.561 \n",
      "train epoch 1 loss : 64.791 \n",
      "train epoch 1 loss : 104.092 \n",
      "train epoch 1 loss : 64.825 \n",
      "train epoch 1 loss : 69.337 \n",
      "train epoch 1 loss : 53.462 \n",
      "train epoch 1 loss : 79.519 \n",
      "train epoch 1 loss : 43.891 \n",
      "train epoch 1 loss : 99.038 \n",
      "train epoch 1 loss : 78.912 \n",
      "train epoch 1 loss : 64.187 \n",
      "train epoch 1 loss : 92.632 \n",
      "train epoch 1 loss : 74.158 \n",
      "train epoch 1 loss : 82.849 \n",
      "train epoch 1 loss : 74.857 \n",
      "train epoch 1 loss : 87.655 \n",
      "train epoch 1 loss : 82.265 \n",
      "train epoch 1 loss : 95.378 \n",
      "train epoch 1 loss : 60.712 \n",
      "train epoch 1 loss : 80.004 \n",
      "train epoch 1 loss : 84.564 \n",
      "train epoch 1 loss : 127.724 \n",
      "train epoch 1 loss : 102.508 \n",
      "train epoch 1 loss : 96.575 \n",
      "train epoch 1 loss : 72.649 \n",
      "train epoch 1 loss : 66.135 \n",
      "train epoch 1 loss : 46.686 \n",
      "train epoch 1 loss : 102.787 \n",
      "train epoch 1 loss : 59.196 \n",
      "train epoch 1 loss : 64.490 \n",
      "train epoch 1 loss : 65.497 \n",
      "train epoch 1 loss : 50.004 \n",
      "train epoch 1 loss : 50.016 \n",
      "train epoch 1 loss : 66.403 \n",
      "train epoch 1 loss : 68.533 \n",
      "train epoch 1 loss : 75.112 \n",
      "train epoch 1 loss : 74.589 \n",
      "train epoch 1 loss : 71.742 \n",
      "train epoch 1 loss : 60.580 \n",
      "train epoch 1 loss : 99.170 \n",
      "train epoch 1 loss : 83.241 \n",
      "train epoch 1 loss : 80.272 \n",
      "train epoch 1 loss : 59.778 \n",
      "train epoch 1 loss : 80.302 \n",
      "train epoch 1 loss : 72.324 \n",
      "train epoch 1 loss : 120.401 \n",
      "train epoch 1 loss : 74.162 \n",
      "train epoch 1 loss : 91.020 \n",
      "train epoch 1 loss : 44.680 \n",
      "train epoch 1 loss : 48.324 \n",
      "train epoch 1 loss : 51.377 \n",
      "train epoch 1 loss : 68.929 \n",
      "train epoch 1 loss : 54.459 \n",
      "train epoch 1 loss : 82.714 \n",
      "train epoch 1 loss : 69.089 \n",
      "train epoch 1 loss : 63.194 \n",
      "train epoch 1 loss : 60.139 \n",
      "train epoch 1 loss : 70.055 \n",
      "train epoch 1 loss : 66.699 \n",
      "train epoch 1 loss : 60.671 \n",
      "train epoch 1 loss : 50.299 \n",
      "train epoch 1 loss : 67.519 \n",
      "train epoch 1 loss : 81.726 \n",
      "train epoch 1 loss : 45.669 \n",
      "train epoch 1 loss : 50.486 \n",
      "train epoch 1 loss : 59.713 \n",
      "train epoch 1 loss : 51.924 \n",
      "train epoch 2 loss : 50.681 \n",
      "train epoch 2 loss : 84.434 \n",
      "train epoch 2 loss : 74.387 \n",
      "train epoch 2 loss : 65.697 \n",
      "train epoch 2 loss : 49.243 \n",
      "train epoch 2 loss : 62.244 \n",
      "train epoch 2 loss : 74.014 \n",
      "train epoch 2 loss : 46.040 \n",
      "train epoch 2 loss : 115.325 \n",
      "train epoch 2 loss : 74.482 \n",
      "train epoch 2 loss : 62.466 \n",
      "train epoch 2 loss : 76.572 \n",
      "train epoch 2 loss : 84.166 \n",
      "train epoch 2 loss : 90.967 \n",
      "train epoch 2 loss : 92.201 \n",
      "train epoch 2 loss : 88.981 \n",
      "train epoch 2 loss : 52.960 \n",
      "train epoch 2 loss : 59.375 \n",
      "train epoch 2 loss : 60.222 \n",
      "train epoch 2 loss : 73.416 \n",
      "train epoch 2 loss : 66.467 \n",
      "train epoch 2 loss : 55.025 \n",
      "train epoch 2 loss : 97.935 \n",
      "train epoch 2 loss : 58.952 \n",
      "train epoch 2 loss : 57.465 \n",
      "train epoch 2 loss : 63.771 \n",
      "train epoch 2 loss : 39.705 \n",
      "train epoch 2 loss : 73.073 \n",
      "train epoch 2 loss : 39.434 \n",
      "train epoch 2 loss : 111.050 \n",
      "train epoch 2 loss : 93.335 \n",
      "train epoch 2 loss : 58.972 \n",
      "train epoch 2 loss : 60.212 \n",
      "train epoch 2 loss : 93.624 \n",
      "train epoch 2 loss : 67.034 \n",
      "train epoch 2 loss : 79.108 \n",
      "train epoch 2 loss : 104.737 \n",
      "train epoch 2 loss : 128.280 \n",
      "train epoch 2 loss : 77.998 \n",
      "train epoch 2 loss : 94.327 \n",
      "train epoch 2 loss : 97.464 \n",
      "train epoch 2 loss : 147.292 \n",
      "train epoch 2 loss : 92.997 \n",
      "train epoch 2 loss : 79.876 \n",
      "train epoch 2 loss : 107.978 \n",
      "train epoch 2 loss : 93.330 \n",
      "train epoch 2 loss : 75.065 \n",
      "train epoch 2 loss : 78.938 \n",
      "train epoch 2 loss : 124.583 \n",
      "train epoch 2 loss : 57.250 \n",
      "train epoch 2 loss : 70.589 \n",
      "train epoch 2 loss : 85.627 \n",
      "train epoch 2 loss : 72.876 \n",
      "train epoch 2 loss : 49.086 \n",
      "train epoch 2 loss : 60.078 \n",
      "train epoch 2 loss : 72.004 \n",
      "train epoch 2 loss : 96.586 \n",
      "train epoch 2 loss : 60.377 \n",
      "train epoch 2 loss : 104.272 \n",
      "train epoch 2 loss : 42.432 \n",
      "train epoch 2 loss : 87.396 \n",
      "train epoch 2 loss : 65.637 \n",
      "train epoch 2 loss : 82.053 \n",
      "train epoch 2 loss : 87.133 \n",
      "train epoch 2 loss : 95.365 \n",
      "train epoch 2 loss : 49.451 \n",
      "train epoch 2 loss : 72.796 \n",
      "train epoch 2 loss : 124.811 \n",
      "train epoch 2 loss : 85.619 \n",
      "train epoch 2 loss : 120.106 \n",
      "train epoch 2 loss : 69.675 \n",
      "train epoch 2 loss : 101.711 \n",
      "train epoch 2 loss : 63.826 \n",
      "train epoch 2 loss : 76.384 \n",
      "train epoch 2 loss : 128.146 \n",
      "train epoch 2 loss : 57.589 \n",
      "train epoch 2 loss : 49.482 \n",
      "train epoch 2 loss : 45.232 \n",
      "train epoch 2 loss : 95.263 \n",
      "train epoch 2 loss : 47.422 \n",
      "train epoch 2 loss : 83.650 \n",
      "train epoch 2 loss : 68.844 \n",
      "train epoch 2 loss : 53.810 \n",
      "train epoch 2 loss : 69.079 \n",
      "train epoch 2 loss : 34.102 \n",
      "train epoch 2 loss : 75.332 \n",
      "train epoch 2 loss : 50.489 \n",
      "train epoch 2 loss : 79.337 \n",
      "train epoch 2 loss : 125.237 \n",
      "train epoch 2 loss : 112.824 \n",
      "train epoch 2 loss : 97.130 \n",
      "train epoch 2 loss : 74.011 \n",
      "train epoch 2 loss : 81.282 \n",
      "train epoch 2 loss : 85.673 \n",
      "train epoch 2 loss : 90.679 \n",
      "train epoch 2 loss : 67.075 \n",
      "train epoch 2 loss : 68.172 \n",
      "train epoch 2 loss : 43.941 \n",
      "train epoch 2 loss : 58.927 \n",
      "train epoch 2 loss : 56.861 \n",
      "train epoch 2 loss : 65.916 \n",
      "train epoch 2 loss : 64.435 \n",
      "train epoch 2 loss : 43.866 \n",
      "train epoch 2 loss : 114.586 \n",
      "train epoch 2 loss : 48.532 \n",
      "train epoch 2 loss : 57.646 \n",
      "train epoch 2 loss : 64.411 \n",
      "train epoch 2 loss : 62.193 \n",
      "train epoch 2 loss : 74.823 \n",
      "train epoch 2 loss : 108.640 \n",
      "train epoch 2 loss : 74.559 \n",
      "train epoch 2 loss : 70.844 \n",
      "train epoch 2 loss : 84.094 \n",
      "train epoch 2 loss : 73.059 \n",
      "train epoch 2 loss : 90.894 \n",
      "train epoch 2 loss : 64.646 \n",
      "train epoch 2 loss : 82.450 \n",
      "train epoch 2 loss : 81.489 \n",
      "train epoch 2 loss : 86.474 \n",
      "train epoch 2 loss : 61.367 \n",
      "train epoch 2 loss : 62.903 \n",
      "train epoch 2 loss : 86.665 \n",
      "train epoch 2 loss : 83.185 \n",
      "train epoch 2 loss : 75.900 \n",
      "train epoch 2 loss : 69.778 \n",
      "train epoch 2 loss : 45.100 \n",
      "train epoch 2 loss : 90.577 \n",
      "train epoch 2 loss : 55.371 \n",
      "train epoch 2 loss : 79.562 \n",
      "train epoch 2 loss : 100.789 \n",
      "train epoch 2 loss : 95.392 \n",
      "train epoch 2 loss : 101.408 \n",
      "train epoch 2 loss : 65.165 \n",
      "train epoch 2 loss : 81.316 \n",
      "train epoch 2 loss : 104.580 \n",
      "train epoch 2 loss : 43.398 \n",
      "train epoch 2 loss : 65.048 \n",
      "train epoch 2 loss : 96.626 \n",
      "train epoch 2 loss : 55.799 \n",
      "train epoch 2 loss : 58.992 \n",
      "train epoch 2 loss : 56.180 \n",
      "train epoch 2 loss : 70.471 \n",
      "train epoch 2 loss : 70.992 \n",
      "train epoch 2 loss : 60.025 \n",
      "train epoch 2 loss : 77.943 \n",
      "train epoch 2 loss : 89.265 \n",
      "train epoch 2 loss : 75.353 \n",
      "train epoch 2 loss : 97.691 \n",
      "train epoch 2 loss : 53.625 \n",
      "train epoch 2 loss : 92.785 \n",
      "train epoch 2 loss : 86.593 \n",
      "train epoch 2 loss : 177.042 \n",
      "train epoch 2 loss : 115.674 \n",
      "train epoch 2 loss : 162.267 \n",
      "train epoch 2 loss : 79.564 \n",
      "train epoch 2 loss : 91.959 \n",
      "train epoch 2 loss : 38.995 \n",
      "train epoch 2 loss : 84.018 \n",
      "train epoch 2 loss : 88.681 \n",
      "train epoch 2 loss : 65.389 \n",
      "train epoch 2 loss : 84.058 \n",
      "train epoch 2 loss : 107.115 \n",
      "train epoch 2 loss : 78.150 \n",
      "train epoch 2 loss : 114.376 \n",
      "train epoch 2 loss : 64.933 \n",
      "train epoch 2 loss : 69.358 \n",
      "train epoch 2 loss : 36.972 \n",
      "train epoch 2 loss : 41.082 \n",
      "train epoch 2 loss : 43.714 \n",
      "train epoch 2 loss : 68.096 \n",
      "train epoch 2 loss : 84.259 \n",
      "train epoch 2 loss : 78.125 \n",
      "train epoch 2 loss : 59.308 \n",
      "train epoch 2 loss : 66.557 \n",
      "train epoch 2 loss : 57.953 \n",
      "train epoch 2 loss : 91.014 \n",
      "train epoch 2 loss : 43.640 \n",
      "train epoch 2 loss : 80.869 \n",
      "train epoch 2 loss : 90.078 \n",
      "train epoch 2 loss : 84.358 \n",
      "train epoch 2 loss : 71.367 \n",
      "train epoch 2 loss : 96.859 \n",
      "train epoch 2 loss : 51.038 \n",
      "train epoch 2 loss : 56.975 \n",
      "train epoch 2 loss : 52.455 \n",
      "train epoch 2 loss : 94.575 \n",
      "train epoch 2 loss : 99.571 \n",
      "train epoch 2 loss : 74.838 \n",
      "train epoch 2 loss : 60.119 \n",
      "train epoch 2 loss : 69.708 \n",
      "train epoch 2 loss : 65.345 \n",
      "train epoch 2 loss : 87.304 \n",
      "train epoch 2 loss : 45.478 \n",
      "train epoch 2 loss : 77.052 \n",
      "train epoch 2 loss : 62.590 \n",
      "train epoch 2 loss : 46.520 \n",
      "train epoch 2 loss : 74.592 \n",
      "train epoch 2 loss : 96.829 \n",
      "train epoch 2 loss : 48.201 \n",
      "train epoch 2 loss : 61.258 \n",
      "train epoch 3 loss : 74.129 \n",
      "train epoch 3 loss : 55.636 \n",
      "train epoch 3 loss : 48.943 \n",
      "train epoch 3 loss : 95.971 \n",
      "train epoch 3 loss : 78.239 \n",
      "train epoch 3 loss : 110.903 \n",
      "train epoch 3 loss : 52.925 \n",
      "train epoch 3 loss : 59.768 \n",
      "train epoch 3 loss : 80.002 \n",
      "train epoch 3 loss : 81.326 \n",
      "train epoch 3 loss : 77.475 \n",
      "train epoch 3 loss : 105.974 \n",
      "train epoch 3 loss : 85.108 \n",
      "train epoch 3 loss : 86.001 \n",
      "train epoch 3 loss : 62.207 \n",
      "train epoch 3 loss : 60.822 \n",
      "train epoch 3 loss : 90.827 \n",
      "train epoch 3 loss : 68.055 \n",
      "train epoch 3 loss : 53.551 \n",
      "train epoch 3 loss : 78.946 \n",
      "train epoch 3 loss : 76.549 \n",
      "train epoch 3 loss : 78.266 \n",
      "train epoch 3 loss : 58.122 \n",
      "train epoch 3 loss : 77.510 \n",
      "train epoch 3 loss : 83.398 \n",
      "train epoch 3 loss : 135.612 \n",
      "train epoch 3 loss : 50.426 \n",
      "train epoch 3 loss : 88.752 \n",
      "train epoch 3 loss : 85.726 \n",
      "train epoch 3 loss : 51.628 \n",
      "train epoch 3 loss : 61.502 \n",
      "train epoch 3 loss : 98.622 \n",
      "train epoch 3 loss : 90.765 \n",
      "train epoch 3 loss : 81.918 \n",
      "train epoch 3 loss : 60.678 \n",
      "train epoch 3 loss : 46.545 \n",
      "train epoch 3 loss : 78.882 \n",
      "train epoch 3 loss : 57.676 \n",
      "train epoch 3 loss : 66.842 \n",
      "train epoch 3 loss : 62.116 \n",
      "train epoch 3 loss : 98.035 \n",
      "train epoch 3 loss : 72.571 \n",
      "train epoch 3 loss : 67.994 \n",
      "train epoch 3 loss : 71.706 \n",
      "train epoch 3 loss : 71.064 \n",
      "train epoch 3 loss : 62.870 \n",
      "train epoch 3 loss : 63.467 \n",
      "train epoch 3 loss : 54.674 \n",
      "train epoch 3 loss : 72.074 \n",
      "train epoch 3 loss : 76.748 \n",
      "train epoch 3 loss : 109.925 \n",
      "train epoch 3 loss : 141.740 \n",
      "train epoch 3 loss : 47.548 \n",
      "train epoch 3 loss : 45.669 \n",
      "train epoch 3 loss : 64.627 \n",
      "train epoch 3 loss : 50.917 \n",
      "train epoch 3 loss : 103.467 \n",
      "train epoch 3 loss : 92.773 \n",
      "train epoch 3 loss : 49.452 \n",
      "train epoch 3 loss : 74.536 \n",
      "train epoch 3 loss : 81.285 \n",
      "train epoch 3 loss : 44.837 \n",
      "train epoch 3 loss : 32.218 \n",
      "train epoch 3 loss : 54.564 \n",
      "train epoch 3 loss : 63.184 \n",
      "train epoch 3 loss : 63.068 \n",
      "train epoch 3 loss : 70.363 \n",
      "train epoch 3 loss : 56.915 \n",
      "train epoch 3 loss : 66.256 \n",
      "train epoch 3 loss : 62.184 \n",
      "train epoch 3 loss : 63.896 \n",
      "train epoch 3 loss : 58.331 \n",
      "train epoch 3 loss : 50.664 \n",
      "train epoch 3 loss : 42.400 \n",
      "train epoch 3 loss : 82.533 \n",
      "train epoch 3 loss : 103.335 \n",
      "train epoch 3 loss : 131.262 \n",
      "train epoch 3 loss : 83.341 \n",
      "train epoch 3 loss : 126.813 \n",
      "train epoch 3 loss : 98.510 \n",
      "train epoch 3 loss : 65.832 \n",
      "train epoch 3 loss : 91.423 \n",
      "train epoch 3 loss : 75.456 \n",
      "train epoch 3 loss : 98.516 \n",
      "train epoch 3 loss : 106.475 \n",
      "train epoch 3 loss : 89.774 \n",
      "train epoch 3 loss : 76.295 \n",
      "train epoch 3 loss : 64.194 \n",
      "train epoch 3 loss : 62.588 \n",
      "train epoch 3 loss : 104.306 \n",
      "train epoch 3 loss : 117.370 \n",
      "train epoch 3 loss : 73.948 \n",
      "train epoch 3 loss : 93.353 \n",
      "train epoch 3 loss : 131.549 \n",
      "train epoch 3 loss : 135.071 \n",
      "train epoch 3 loss : 59.693 \n",
      "train epoch 3 loss : 52.376 \n",
      "train epoch 3 loss : 67.358 \n",
      "train epoch 3 loss : 60.680 \n",
      "train epoch 3 loss : 50.359 \n",
      "train epoch 3 loss : 97.919 \n",
      "train epoch 3 loss : 93.756 \n",
      "train epoch 3 loss : 68.716 \n",
      "train epoch 3 loss : 71.353 \n",
      "train epoch 3 loss : 73.786 \n",
      "train epoch 3 loss : 74.191 \n",
      "train epoch 3 loss : 54.728 \n",
      "train epoch 3 loss : 58.287 \n",
      "train epoch 3 loss : 83.506 \n",
      "train epoch 3 loss : 38.363 \n",
      "train epoch 3 loss : 48.797 \n",
      "train epoch 3 loss : 97.326 \n",
      "train epoch 3 loss : 61.773 \n",
      "train epoch 3 loss : 75.179 \n",
      "train epoch 3 loss : 46.099 \n",
      "train epoch 3 loss : 75.791 \n",
      "train epoch 3 loss : 74.807 \n",
      "train epoch 3 loss : 64.354 \n",
      "train epoch 3 loss : 59.128 \n",
      "train epoch 3 loss : 61.502 \n",
      "train epoch 3 loss : 63.375 \n",
      "train epoch 3 loss : 38.050 \n",
      "train epoch 3 loss : 59.973 \n",
      "train epoch 3 loss : 60.242 \n",
      "train epoch 3 loss : 110.077 \n",
      "train epoch 3 loss : 58.399 \n",
      "train epoch 3 loss : 90.762 \n",
      "train epoch 3 loss : 85.690 \n",
      "train epoch 3 loss : 101.346 \n",
      "train epoch 3 loss : 97.026 \n",
      "train epoch 3 loss : 62.827 \n",
      "train epoch 3 loss : 67.597 \n",
      "train epoch 3 loss : 53.818 \n",
      "train epoch 3 loss : 75.022 \n",
      "train epoch 3 loss : 90.875 \n",
      "train epoch 3 loss : 110.637 \n",
      "train epoch 3 loss : 55.618 \n",
      "train epoch 3 loss : 80.396 \n",
      "train epoch 3 loss : 60.842 \n",
      "train epoch 3 loss : 45.570 \n",
      "train epoch 3 loss : 82.962 \n",
      "train epoch 3 loss : 62.898 \n",
      "train epoch 3 loss : 54.466 \n",
      "train epoch 3 loss : 79.505 \n",
      "train epoch 3 loss : 58.347 \n",
      "train epoch 3 loss : 69.684 \n",
      "train epoch 3 loss : 62.752 \n",
      "train epoch 3 loss : 53.994 \n",
      "train epoch 3 loss : 58.224 \n",
      "train epoch 3 loss : 53.748 \n",
      "train epoch 3 loss : 54.950 \n",
      "train epoch 3 loss : 60.178 \n",
      "train epoch 3 loss : 48.047 \n",
      "train epoch 3 loss : 96.849 \n",
      "train epoch 3 loss : 49.306 \n",
      "train epoch 3 loss : 54.425 \n",
      "train epoch 3 loss : 85.726 \n",
      "train epoch 3 loss : 68.647 \n",
      "train epoch 3 loss : 44.606 \n",
      "train epoch 3 loss : 73.150 \n",
      "train epoch 3 loss : 80.207 \n",
      "train epoch 3 loss : 95.260 \n",
      "train epoch 3 loss : 71.511 \n",
      "train epoch 3 loss : 65.330 \n",
      "train epoch 3 loss : 65.044 \n",
      "train epoch 3 loss : 82.753 \n",
      "train epoch 3 loss : 98.520 \n",
      "train epoch 3 loss : 73.768 \n",
      "train epoch 3 loss : 76.236 \n",
      "train epoch 3 loss : 60.362 \n",
      "train epoch 3 loss : 81.633 \n",
      "train epoch 3 loss : 88.975 \n",
      "train epoch 3 loss : 142.755 \n",
      "train epoch 3 loss : 81.425 \n",
      "train epoch 3 loss : 97.649 \n",
      "train epoch 3 loss : 86.717 \n",
      "train epoch 3 loss : 55.463 \n",
      "train epoch 3 loss : 52.019 \n",
      "train epoch 3 loss : 66.857 \n",
      "train epoch 3 loss : 81.602 \n",
      "train epoch 3 loss : 49.017 \n",
      "train epoch 3 loss : 73.236 \n",
      "train epoch 3 loss : 83.567 \n",
      "train epoch 3 loss : 165.275 \n",
      "train epoch 3 loss : 129.842 \n",
      "train epoch 3 loss : 229.716 \n",
      "train epoch 3 loss : 121.573 \n",
      "train epoch 3 loss : 110.331 \n",
      "train epoch 3 loss : 184.179 \n",
      "train epoch 3 loss : 54.281 \n",
      "train epoch 3 loss : 57.810 \n",
      "train epoch 3 loss : 64.942 \n",
      "train epoch 3 loss : 84.417 \n",
      "train epoch 3 loss : 59.215 \n",
      "train epoch 3 loss : 53.483 \n",
      "train epoch 3 loss : 58.115 \n",
      "train epoch 3 loss : 71.869 \n",
      "train epoch 3 loss : 95.853 \n",
      "train epoch 3 loss : 65.015 \n",
      "train epoch 3 loss : 102.832 \n",
      "train epoch 4 loss : 89.434 \n",
      "train epoch 4 loss : 90.888 \n",
      "train epoch 4 loss : 48.910 \n",
      "train epoch 4 loss : 38.334 \n",
      "train epoch 4 loss : 26.922 \n",
      "train epoch 4 loss : 54.992 \n",
      "train epoch 4 loss : 79.447 \n",
      "train epoch 4 loss : 67.008 \n",
      "train epoch 4 loss : 69.383 \n",
      "train epoch 4 loss : 44.131 \n",
      "train epoch 4 loss : 82.362 \n",
      "train epoch 4 loss : 59.804 \n",
      "train epoch 4 loss : 43.005 \n",
      "train epoch 4 loss : 50.564 \n",
      "train epoch 4 loss : 51.471 \n",
      "train epoch 4 loss : 62.733 \n",
      "train epoch 4 loss : 74.382 \n",
      "train epoch 4 loss : 94.436 \n",
      "train epoch 4 loss : 93.864 \n",
      "train epoch 4 loss : 76.019 \n",
      "train epoch 4 loss : 62.586 \n",
      "train epoch 4 loss : 61.434 \n",
      "train epoch 4 loss : 64.222 \n",
      "train epoch 4 loss : 66.096 \n",
      "train epoch 4 loss : 65.185 \n",
      "train epoch 4 loss : 53.509 \n",
      "train epoch 4 loss : 54.857 \n",
      "train epoch 4 loss : 72.347 \n",
      "train epoch 4 loss : 75.451 \n",
      "train epoch 4 loss : 58.162 \n",
      "train epoch 4 loss : 44.135 \n",
      "train epoch 4 loss : 82.100 \n",
      "train epoch 4 loss : 55.121 \n",
      "train epoch 4 loss : 94.493 \n",
      "train epoch 4 loss : 69.135 \n",
      "train epoch 4 loss : 86.087 \n",
      "train epoch 4 loss : 47.426 \n",
      "train epoch 4 loss : 77.187 \n",
      "train epoch 4 loss : 77.564 \n",
      "train epoch 4 loss : 91.164 \n",
      "train epoch 4 loss : 66.302 \n",
      "train epoch 4 loss : 53.920 \n",
      "train epoch 4 loss : 55.717 \n",
      "train epoch 4 loss : 51.571 \n",
      "train epoch 4 loss : 57.345 \n",
      "train epoch 4 loss : 53.063 \n",
      "train epoch 4 loss : 89.200 \n",
      "train epoch 4 loss : 68.901 \n",
      "train epoch 4 loss : 62.867 \n",
      "train epoch 4 loss : 63.706 \n",
      "train epoch 4 loss : 67.385 \n",
      "train epoch 4 loss : 92.960 \n",
      "train epoch 4 loss : 91.938 \n",
      "train epoch 4 loss : 56.195 \n",
      "train epoch 4 loss : 48.164 \n",
      "train epoch 4 loss : 63.967 \n",
      "train epoch 4 loss : 49.244 \n",
      "train epoch 4 loss : 66.501 \n",
      "train epoch 4 loss : 59.001 \n",
      "train epoch 4 loss : 49.832 \n",
      "train epoch 4 loss : 58.686 \n",
      "train epoch 4 loss : 46.902 \n",
      "train epoch 4 loss : 65.962 \n",
      "train epoch 4 loss : 72.890 \n",
      "train epoch 4 loss : 65.351 \n",
      "train epoch 4 loss : 77.085 \n",
      "train epoch 4 loss : 68.956 \n",
      "train epoch 4 loss : 104.272 \n",
      "train epoch 4 loss : 112.431 \n",
      "train epoch 4 loss : 50.624 \n",
      "train epoch 4 loss : 57.965 \n",
      "train epoch 4 loss : 78.122 \n",
      "train epoch 4 loss : 54.687 \n",
      "train epoch 4 loss : 59.505 \n",
      "train epoch 4 loss : 124.197 \n",
      "train epoch 4 loss : 109.108 \n",
      "train epoch 4 loss : 87.412 \n",
      "train epoch 4 loss : 91.438 \n",
      "train epoch 4 loss : 72.099 \n",
      "train epoch 4 loss : 41.330 \n",
      "train epoch 4 loss : 51.409 \n",
      "train epoch 4 loss : 56.089 \n",
      "train epoch 4 loss : 72.523 \n",
      "train epoch 4 loss : 86.569 \n",
      "train epoch 4 loss : 55.804 \n",
      "train epoch 4 loss : 73.064 \n",
      "train epoch 4 loss : 69.360 \n",
      "train epoch 4 loss : 49.973 \n",
      "train epoch 4 loss : 53.454 \n",
      "train epoch 4 loss : 33.323 \n",
      "train epoch 4 loss : 49.124 \n",
      "train epoch 4 loss : 51.087 \n",
      "train epoch 4 loss : 53.269 \n",
      "train epoch 4 loss : 57.409 \n",
      "train epoch 4 loss : 31.178 \n",
      "train epoch 4 loss : 60.433 \n",
      "train epoch 4 loss : 53.409 \n",
      "train epoch 4 loss : 42.960 \n",
      "train epoch 4 loss : 50.694 \n",
      "train epoch 4 loss : 58.602 \n",
      "train epoch 4 loss : 100.140 \n",
      "train epoch 4 loss : 73.837 \n",
      "train epoch 4 loss : 79.188 \n",
      "train epoch 4 loss : 93.565 \n",
      "train epoch 4 loss : 109.795 \n",
      "train epoch 4 loss : 74.065 \n",
      "train epoch 4 loss : 64.354 \n",
      "train epoch 4 loss : 40.719 \n",
      "train epoch 4 loss : 41.147 \n",
      "train epoch 4 loss : 45.045 \n",
      "train epoch 4 loss : 53.736 \n",
      "train epoch 4 loss : 46.824 \n",
      "train epoch 4 loss : 67.257 \n",
      "train epoch 4 loss : 55.794 \n",
      "train epoch 4 loss : 56.321 \n",
      "train epoch 4 loss : 120.404 \n",
      "train epoch 4 loss : 88.272 \n",
      "train epoch 4 loss : 42.537 \n",
      "train epoch 4 loss : 57.487 \n",
      "train epoch 4 loss : 59.790 \n",
      "train epoch 4 loss : 67.861 \n",
      "train epoch 4 loss : 50.077 \n",
      "train epoch 4 loss : 42.840 \n",
      "train epoch 4 loss : 39.198 \n",
      "train epoch 4 loss : 51.911 \n",
      "train epoch 4 loss : 26.850 \n",
      "train epoch 4 loss : 35.034 \n",
      "train epoch 4 loss : 68.995 \n",
      "train epoch 4 loss : 65.729 \n",
      "train epoch 4 loss : 44.410 \n",
      "train epoch 4 loss : 90.832 \n",
      "train epoch 4 loss : 39.369 \n",
      "train epoch 4 loss : 46.237 \n",
      "train epoch 4 loss : 61.331 \n",
      "train epoch 4 loss : 51.153 \n",
      "train epoch 4 loss : 45.767 \n",
      "train epoch 4 loss : 61.096 \n",
      "train epoch 4 loss : 51.429 \n",
      "train epoch 4 loss : 40.889 \n",
      "train epoch 4 loss : 49.649 \n",
      "train epoch 4 loss : 48.466 \n",
      "train epoch 4 loss : 50.005 \n",
      "train epoch 4 loss : 43.556 \n",
      "train epoch 4 loss : 55.382 \n",
      "train epoch 4 loss : 46.332 \n",
      "train epoch 4 loss : 50.790 \n",
      "train epoch 4 loss : 43.905 \n",
      "train epoch 4 loss : 49.207 \n",
      "train epoch 4 loss : 56.329 \n",
      "train epoch 4 loss : 54.837 \n",
      "train epoch 4 loss : 62.258 \n",
      "train epoch 4 loss : 60.248 \n",
      "train epoch 4 loss : 34.521 \n",
      "train epoch 4 loss : 51.374 \n",
      "train epoch 4 loss : 49.292 \n",
      "train epoch 4 loss : 41.248 \n",
      "train epoch 4 loss : 56.022 \n",
      "train epoch 4 loss : 47.197 \n",
      "train epoch 4 loss : 50.768 \n",
      "train epoch 4 loss : 56.011 \n",
      "train epoch 4 loss : 61.929 \n",
      "train epoch 4 loss : 70.726 \n",
      "train epoch 4 loss : 38.691 \n",
      "train epoch 4 loss : 60.587 \n",
      "train epoch 4 loss : 64.960 \n",
      "train epoch 4 loss : 45.715 \n",
      "train epoch 4 loss : 37.376 \n",
      "train epoch 4 loss : 49.619 \n",
      "train epoch 4 loss : 43.852 \n",
      "train epoch 4 loss : 43.346 \n",
      "train epoch 4 loss : 56.825 \n",
      "train epoch 4 loss : 28.210 \n",
      "train epoch 4 loss : 38.265 \n",
      "train epoch 4 loss : 27.037 \n",
      "train epoch 4 loss : 44.691 \n",
      "train epoch 4 loss : 32.487 \n",
      "train epoch 4 loss : 25.662 \n",
      "train epoch 4 loss : 40.095 \n",
      "train epoch 4 loss : 42.567 \n",
      "train epoch 4 loss : 37.821 \n",
      "train epoch 4 loss : 47.973 \n",
      "train epoch 4 loss : 61.507 \n",
      "train epoch 4 loss : 56.745 \n",
      "train epoch 4 loss : 30.425 \n",
      "train epoch 4 loss : 48.359 \n",
      "train epoch 4 loss : 63.223 \n",
      "train epoch 4 loss : 56.804 \n",
      "train epoch 4 loss : 47.204 \n",
      "train epoch 4 loss : 57.241 \n",
      "train epoch 4 loss : 43.500 \n",
      "train epoch 4 loss : 51.298 \n",
      "train epoch 4 loss : 41.821 \n",
      "train epoch 4 loss : 37.096 \n",
      "train epoch 4 loss : 30.050 \n",
      "train epoch 4 loss : 74.219 \n",
      "train epoch 4 loss : 31.196 \n",
      "train epoch 4 loss : 71.615 \n",
      "train epoch 4 loss : 25.133 \n",
      "train epoch 4 loss : 64.935 \n",
      "train epoch 4 loss : 51.493 \n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr =0.001)\n",
    "criteon= nn.MSELoss()\n",
    "EPOCH = 5\n",
    "loss_list = []\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()\n",
    "    sum_loss = 0 \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        #data, target = Variable(data), Variable(target)\n",
    "\n",
    "        data = torch.reshape(data,(batchsize, 1, 32,32))\n",
    "        target = torch.reshape(target, (batchsize, 1))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        data=data.to(torch.float32)\n",
    "        target=target.to(torch.float32)\n",
    "        output = model(data)\n",
    "\n",
    "        \n",
    "        #print(data)\n",
    "        #print(target)\n",
    "        #print(\"The output is:\")\n",
    "        #print(output)\n",
    "        \n",
    "        loss = criteon(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "        if batch_idx % 10 == 9:\n",
    "            print(\"train epoch {} loss : {:.3f} \".format(epoch, sum_loss/10))\n",
    "            loss_list.append(sum_loss/100)\n",
    "            sum_loss = 0      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 74.59051  ,   6.0666823,  53.21632  , ...,   0.5124746,\n",
       "       -12.01869  ,  -2.283545 ], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.cpu()\n",
    "re = model(torch.Tensor(X_test.reshape(4000,1,32,32))).detach().numpy()\n",
    "re = re.reshape(4000)\n",
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 61.5076,   4.2478,  48.3385,  ...,   3.4108, -27.6324,  -7.8302])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74.590508</td>\n",
       "      <td>61.507622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.066682</td>\n",
       "      <td>4.247818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53.216320</td>\n",
       "      <td>48.338482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.949986</td>\n",
       "      <td>23.041401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.976379</td>\n",
       "      <td>68.316994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>-1.680231</td>\n",
       "      <td>-6.869918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>1.164764</td>\n",
       "      <td>4.799513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>0.512475</td>\n",
       "      <td>3.410830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>-12.018690</td>\n",
       "      <td>-27.632389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>-2.283545</td>\n",
       "      <td>-7.830187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pre       true\n",
       "0     74.590508  61.507622\n",
       "1      6.066682   4.247818\n",
       "2     53.216320  48.338482\n",
       "3     38.949986  23.041401\n",
       "4     75.976379  68.316994\n",
       "...         ...        ...\n",
       "3995  -1.680231  -6.869918\n",
       "3996   1.164764   4.799513\n",
       "3997   0.512475   3.410830\n",
       "3998 -12.018690 -27.632389\n",
       "3999  -2.283545  -7.830187\n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultvalue = pd.DataFrame([re,Y_test.numpy()]).T\n",
    "resultvalue.columns = ['pre','true']\n",
    "resultvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre</th>\n",
       "      <th>true</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74.590508</td>\n",
       "      <td>61.507622</td>\n",
       "      <td>0.212703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.066682</td>\n",
       "      <td>4.247818</td>\n",
       "      <td>0.428188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53.216320</td>\n",
       "      <td>48.338482</td>\n",
       "      <td>0.100910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.949986</td>\n",
       "      <td>23.041401</td>\n",
       "      <td>0.690435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.976379</td>\n",
       "      <td>68.316994</td>\n",
       "      <td>0.112115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>-1.680231</td>\n",
       "      <td>-6.869918</td>\n",
       "      <td>0.755422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>1.164764</td>\n",
       "      <td>4.799513</td>\n",
       "      <td>0.757316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>0.512475</td>\n",
       "      <td>3.410830</td>\n",
       "      <td>0.849751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>-12.018690</td>\n",
       "      <td>-27.632389</td>\n",
       "      <td>0.565051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>-2.283545</td>\n",
       "      <td>-7.830187</td>\n",
       "      <td>0.708366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pre       true      mape\n",
       "0     74.590508  61.507622  0.212703\n",
       "1      6.066682   4.247818  0.428188\n",
       "2     53.216320  48.338482  0.100910\n",
       "3     38.949986  23.041401  0.690435\n",
       "4     75.976379  68.316994  0.112115\n",
       "...         ...        ...       ...\n",
       "3995  -1.680231  -6.869918  0.755422\n",
       "3996   1.164764   4.799513  0.757316\n",
       "3997   0.512475   3.410830  0.849751\n",
       "3998 -12.018690 -27.632389  0.565051\n",
       "3999  -2.283545  -7.830187  0.708366\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultvalue['mape'] = np.abs(resultvalue['pre'] - resultvalue['true'])/np.abs(resultvalue['true'])\n",
    "resultvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21775773173050655"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultvalue['mape'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultvalue.to_csv(\"hhh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
